{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import load_csv_data, predict_labels, create_csv_submission\n",
    "from implementations import least_squares_GD, least_squares_SGD, least_squares, ridge_regression, batch_iter, ridge_GD, ridge_SGD\n",
    "from preprocessing import standardize_train, standardize_test, add_bias\n",
    "from losses_gradients import compute_loss_ls, compute_gradient_least_squares, compute_loss_ridge, compute_gradient_ridge\n",
    "from plots import plot_train_test\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, input_train, ids_train = load_csv_data('train.csv', sub_sample=False)\n",
    "y_test, input_test, ids_test = load_csv_data('test.csv', sub_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEE: on pourrait ausi tenter build_polynomial comme dans les séries si on est motivés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise data\n",
    "# Careful to standardize the x_test with the mean and std of x_train\n",
    "x_train, mean, std = standardize_train(input_train)\n",
    "x_train = add_bias(x_train)\n",
    "x_test = standardize_test(input_test, mean, std)\n",
    "x_test = add_bias(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_fold = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm. (probably change afterwards)\n",
    "max_iters = 200   #les plots sont moches parce que j'ai fait avec 20 ici                                  \n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.random.rand(x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For grid search of hyperparameters\n",
    "num_intervals = 15\n",
    "gammas = np.linspace(0, 0.90, num_intervals)\n",
    "lambdas = np.logspace(-4, -0.05, num_intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GD Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_gd_hyperparam(gammas, nb_fold, x_train, y_train):\n",
    "    loss_valid = np.zeros([len(gammas), nb_cross_validation])\n",
    "    loss_train = np.zeros([len(gammas), nb_cross_validation])\n",
    "    \n",
    "    nb_elem = math.floor(x_train.shape[0]/nb_cross_validation)\n",
    "    \n",
    "    for i, gamma in enumerate(gammas):\n",
    "        for k in range(nb_fold):\n",
    "            x_valid_k = x_train[k*nb_elem:(k+1)*nb_elem][:]  \n",
    "            y_valid_k = y_train[k*nb_elem:(k+1)*nb_elem]\n",
    "            \n",
    "            x_train_k = np.concatenate([x_train[0:k*nb_elem][:], x_train[(k+1)*nb_elem:][:]])\n",
    "            y_train_k = np.concatenate([y_train[0:k*nb_elem],    y_train[(k+1)*nb_elem:]   ]) \n",
    "                                        \n",
    "            w, loss_tr = least_squares_GD(y_train_k, x_train_k, w_initial, max_iters, gamma)\n",
    "            loss_train[i][k] = loss_tr\n",
    "            loss_valid[i][k] = compute_loss_ls(y_valid_k, x_valid_k, w)\n",
    "            \n",
    "    return loss_valid, loss_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pauli\\MLProjet\\losses_gradients.py:8: RuntimeWarning: overflow encountered in square\n",
      "  return (1/2)*np.mean(e**2)\n"
     ]
    }
   ],
   "source": [
    "loss_valid_gd, loss_train_gd = ls_gd_hyperparam(gammas, nb_cross_validation, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gd_mean = np.mean(loss_train_gd, axis=1)\n",
    "valid_gd_mean = np.mean(loss_valid_gd, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEaCAYAAAD+E0veAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3XuYHVWZ7/HvL5107vfuXCDEQLhoSCCEJogoF41IGA+MgygoMwcEYzyDisxxBmd8BGF4BJ1zZBQQM8jljAgDIooMDogDIpcIAULIRSAEJKE3SXduJCG3Tr/nj6q0m053upN07V3d+/d5nnqyq2pV1Vu9d/a716patRQRmJmZAfQqdwBmZpYfTgpmZtbCScHMzFo4KZiZWQsnBTMza+GkYGZmLZwUzMyshZOClY2k1yXNKOHxQtLBpTpeHkiqk3S/pLWS1klaLOkqScPT9edJ2iFpYzq9JukWSYeWO3YrDycFsxyR1LsL9/UB4FHgCeC9ETEMOBVoAo4sKvpURAwChgIzgM3As5Imd1Us1n04KVguSfq4pPnpr9snJR1RtO5SSa9K2pD+8v1E0bqDJf1O0npJjZL+I13+WFrkhfQX8afbOGab26brPirpj+m669JyF6brLpf0k6KyE9JaSe90/nxJS9J4l0n6QlHZkyStkPQPkt4CbunE+f+DpDfT/b0k6SPt/Bm/A9wSEd+OiJUAEfFGRFwWEY+2LhwROyLi1Yj4X8DvgMvbe3+s53JSsNyRNA24GfgCMBL4EXCfpL5pkVeBD5H8sv0W8BNJY9N1VwIPAcOBccAPACLihHT9kRExKCJavvCLtLmtpBrgHuAbQE16/OP34JRWAR8HhgDnA99Lz3GnMcAI4D3ArN2dv6TDgIuAYyJiMPAx4PXWB5Q0EDgujXtv/Jzkb2wVplsmBUk3S1olaWEnyl6S/ppcIOm3kt5TtO4aSQvT6dNFyy+StDT9tVeT1XlYuz4P/Cgi/pD+er0N2Aq8HyAi7o6I+ohoTr/cXwGmp9tuJ/ly3S8itkTE43tw3Pa2PQ1YHBE/i4jtwLXAW53daUT8Z/oLPCLidySJp/gLtxm4LCK2RsTmDs5/B9AXmCSpT0S8HhGvtnHY4ST/v1vilPSdtOaxSdI3Ogi7niRRWYXplkkBuJWkbbQzngfqIuII4GckVWok/QUwDZgKHAt8TdKQdJsnSNpW/9SFMVvnvQf4u/QLbJ2kdcABwH4Akv6mqGllHTCZ5Bc8wN8DAp6WtEjS5/bguO1tux+wfGehSJ4iubyN7dskaaakuZLWpPGeVhQvQENEbOnM+UfEUuBikqadVZLulLRfG4ddS5JsdtagiIi/T68r3At0dO1if2BNZ8/Reo5umRQi4jFafWAlTZT0X5KelfR7Se9Nyz4SEe+kxeaSNAsATAJ+FxFNEbEJeIE00UTE8xHxeinOxdq0HLgqIoYVTQMi4o60pvdvJE0oI9MvuYUkX+ZExFsR8fmI2I+k+eUGdfKOo91sWyD5UgZAkorngU3AgKL5MUVl+5I04fwLMDqN94Gd8e48dGfPP43zpxHxQZLkEcA1bZzLJuAPwF915tzb8Ang93u5rXVj3TIptGMO8KWIOBr438ANbZS5APh1+voFYKakAWkT0cm8+z+6lUYfSf2Kpt4kX/qzJR2rxEBJfyFpMDCQ5IuwAZKLuCQ1BdL5syTtTPxr07I70vmVwEHtBbKbbf8TOFzSX6XxfZmiL35gPnCCpPGShgJfL1pXTdLc0wA0SZoJnNLB36Td85d0mKQPp8lmC8mdQjva2c/fA59TcmF+VHqO44AD2zn/KkkHSvoBcBLJ9RqrMD0iKUgaBHwAuFvSfJILc2NblTkXqAO+CxARD5H8YnsSuAN4iuRWPSutB0i+2HZOl0fEPJJ29etIvpyXAucBRMRi4P+QvF8rgSkkzX07HQP8QdJG4D7gKxHxWrrucuC2tEnmU23E0ua2EdEInAVcDawGDik+ZkT8BvgPYAHwLHB/0boNJEnkrvRcPpPuu127O3+SBHM10EhyvWAU8I/t7Odx4MPACcDLaTPUf5HcpvqDoqLHpef8drpuCMmF7Bd3F6f1TOqug+xImgDcHxGT02sBL0XE2HbKziD5T3BiRKxqp8xPgZ9ExANFy14nuR7R2MXhWzcn6VGSz8tN5Y7FrCv1iJpCRLwNvCbpLEjafCUdmb4+iqTmcHpxQkiryiPT10cAR5DcFWJmVrG6ZVKQtLO55zAlHX8uAD4LXCDpBWARcEZa/LvAINKmJUk7q+59gN9LWkxyPeLciGhK9/9lSStILkovkORfg2ZWEbpt85GZmXW9bllTMDOzbDgpmJlZiy57ImOp1NTUxIQJE8odhplZt/Lss882RkRtR+W6XVKYMGEC8+bNK3cYZmbdiqROPbbHzUdmZtbCScHMzFo4KZiZWQsnBTMza+GkYGbWHRQKcOKJ8Fanx3faK04KZmbdwKZLr6T5scfZdOkVmR7HScHMLM/69weJgf/vh/SimYG3/RCkZHkGnBTMrKKsXr2aqVOnMnXqVMaMGcP+++/fMr9t27ZO7eP888/npZdeyjjSxIGxjNs5p2V4vk0M4Cd8lgktw4R0rcySgqSbJa2StHA3ZU5Kn1y6SNLvsorFzLq3rmxOHzlyJPPnz2f+/PnMnj2br371qy3z1dXVAEQEzc3N7e7jlltu4bDDDtv3YIo0NTW1Of/ka2PZb0JfBGynN/3YwphDhjD39TGdinVPZVlTuJV0zOO2SBpGMmTm6RFxOMnIVmZmu7jySnj8cbgiw+b0pUuXMnnyZGbPns20adMoFArMmjWLuro6Dj/8cK4oOvgHP/hB5s+fT1NTE8OGDePSSy/lyCOP5LjjjmPVql3H8dq4cSPnnXce06dP56ijjuJXv/oVADfddBNnn302H//4x5k5cyYPP/wwM2bM4Oyzz+aoo44C4N///TvMWn43k4GZ+gt+xGw2bV7KjBnvjrWrZPaYi4h4LB0drT2fAX4eEW+k5dscEc3Meq6LL4b589tf//vfQ/GP4B/+MJl69YIPfajtbaZOhWuv3bt4Fi9ezC233MKNN94IwNVXX82IESNoamri5JNP5pOf/CSTJk161zbr16/nxBNP5Oqrr+aSSy7h5ptv5tJLL31XmSuuuIJTTz2VW2+9lbVr13Lsscfy0Y9+FICnnnqK+fPnM3z4cB5++GHmzp3L4sWLGT9+PE8//TS33347px12O/+8+C953/D5/G7GFaxdO4DFDx/6rli7SjmvKRwKDJf0qKRnJf1NewUlzZI0T9K8hoaGEoZoZuU0fTqMGpUkAUj+HTUKjj02m+NNnDiRY445pmX+jjvuYNq0aUybNo0lS5awePHiXbbp378/M2fOBODoo4/m9ddf36XMQw89xFVXXcXUqVM5+eST2bJlC2+88QYAp5xyCsOHD28pe9xxxzF+/HgAfv/733PmmWfy2ZPWMhg47SMzOfHEx7nhhl1j7SrlfCBeb+Bo4CNAf+ApSXMj4uXWBSNiDsnoaNTV1XlUILMeojO/6L/4RZgzB/r1g23b4Mwz4YYbsoln4MCBLa9feeUV/vVf/5Wnn36aYcOGce6557Jly5Zdttl5HQKgqqpql2sDkLT7/+IXv2DixInvWv7YY4+965itY9g5CFrTn+oB6F87qM1yXamcNYUVwH9FxKaIaAQeA44sYzxmlkMrV8Ls2TB3bvJvxn23Wrz99tsMHjyYIUOGUCgUePDBB/d6Xx/72Mf4/ve/3zL//PPPd2q7E044gXvvvZetby5nOUP5zSP/yYfaazfrIuWsKfwSuE5Sb6AaOBb4XhnjMbMc+vnP//z6+utLd9xp06YxadIkJk+ezEEHHcTxxx+/1/u67LLLuPjii5kyZQrNzc0cfPDB/PKXv+xwu+nTp3POOedw4TeuooqtfOmLX2TKlCksXbp0r2PpSGZjNEu6AzgJqAFWApcBfQAi4sa0zNeA84Fm4KaI6LAyWVdXFx5PwcwqyaJhx/NO9OOY9b/d631IejYi6joql+XdR+d0osx3ge9mFYOZWU8w9J16GkfvfU1lT7hHs5lZnkVQs73Atpr9SnI4JwUzsxzbunId/dhKjB1bkuM5KZiZ5VjjguR21D4HOCmYmVW89X9MHmHRf6Kbj8zMKt6mpUlSGHKYawpmZl2uKx6dDXDzzTfzVgl60m1PezPXTHFSMDNLdOGzszvz6OzO2Nek0N6jsluLQoG3GUzNhEG7LddVytmj2cysc4qfnZ3Vg4+A2267jeuvv55t27bxgQ98gOuuu47m5mbOP/985s+fT0Qwa9YsRo8ezfz58/n0pz9N//79efrpp9+VUF555RUuuugiGhsbGThwIDfddBOHHnoo5557LqNHj+a5557jmGOOobq6moaGBpYtW8aYMWOYM2cOs2fP5rnnnqNPnz5ce+219GsocF2vASz4zNls3LiRrVu38pvf/Cazv4GTgpmVT46enb1w4ULuvfdennzySXr37s2sWbO48847mThxIo2Njbz44osArFu3jmHDhvGDH/yA6667jqlTp+6yr1mzZnHTTTcxceJEnnjiCS666CIeeughAF599VV++9vf0qtXL77xjW/w/PPP89hjj9GvXz+uueYaqqurefHFF1m0aBGnnXYav1g/js19hr7rEdtZclIws/yaPh2WLYPGxiQ59OoFNTXQ6mmjXeHhhx/mmWeeoa4ueRLE5s2bOeCAA/jYxz7GSy+9xFe+8hVOO+00TjnllN3uZ926dcydO5czzzyzZVlxk89ZZ51Fr15/brk/44wz6NevHwCPP/44X/va1wA4/PDD2W+//VhdWM6WgWM55ZQjMk8I4KRgZuWUo2dnRwSf+9znuPLKK3dZt2DBAn7961/z/e9/n3vuuYc5c+bsdj81NTXMb6cG1JlHZRcbtr2BHQPfm9mjslvzhWYzy7cSPTt7xowZ3HXXXTQ2NgLJXUpvvPEGDQ0NRARnnXUW3/rWt3juuecAGDx4MBs2bNhlP8OHD2fs2LHce++9ADQ3N/PCCy90KoYTTjiB22+/HYAlS5ZQePNNjmALDB3aFafYKa4pmFm+lejZ2VOmTOGyyy5jxowZNDc306dPH2688Uaqqqq44IILiAgkcc011wBw/vnnc+GFF7Z5ofnOO+/ki1/8Ipdffjnbtm3j3HPP5cgjOx4u5ktf+hJf+MIXmDJlCn369OF7l1xF9Vf/Bo0oXVLI7NHZWfGjs82sUiy54RHe97cf5qmr/pvj/vHkfdpXZx+d7eYjM7OcKnVvZsgwKUi6WdIqSQs7KHeMpB2SPplVLGZm3dG215PezCMn94CkANwKnLq7ApKqgGuAvR/81Mysh4r6ApsYQO3EISU7ZmZJISIeA9Z0UOxLwD3AqqziMDPrrno3FFhVNZaq3irZMct2TUHS/sAngBs7UXaWpHmS5jU0NGQfnJlZDvRfV8+6fqVrOoLyXmi+FviHiNjRUcGImBMRdRFRV1tbW4LQzMzKb8imAhuHlGYchZ3K2U+hDrhTEkANcJqkpoj4RRljMjPLjZHbCiwdObOkxyxbUoiIA3e+lnQrcL8TgplZomndRgbHBmJ0aZuPMksKku4ATgJqJK0ALgP6AEREh9cRzMwqWeOLBcYAVQf0kOajiDhnD8qel1UcZmbd0bolSVLod2DlXGg2M7N2bHw56bg2+FAnBTOzirftT8kjLkrZmxmcFMzMcqn5zQJb6Muow7IfWKeYk4KZWQ71biiwqtdYevcpXW9mcFIwM8ul/mvrWVvi3szgpGBmlkuDNxXYMLi0t6OCk4KZWS6N2Fpg60jXFMzMKt6OjZsZFutoHuWkYGZW8VYvTG5H7TXOzUdmZhVv7eIkKfQ/yDUFM7OKt7M386BDnBTMzCre1teTmsKIyW4+MjOreM1vFthGH0a9b2TJj+2kYGaWM1Wr6lnVawzVfUvbmxmcFMzMcqff2gJr+pW+6QgyTAqSbpa0StLCdtZ/VtKCdHpS0pFZxWJm1p0M3lhgw6DSX2SGbGsKtwKn7mb9a8CJEXEEcCUwJ8NYzMy6jZFb6tk6vDxJIcuR1x6TNGE3658smp0LjMsqFjOz7qJ581aGxxqaSzw28055uaZwAfDrcgdhZlZuaxa/BYDK0JsZMqwpdJakk0mSwgd3U2YWMAtg/PjxJYrMzKz01iwqUAP0m1CBNQVJRwA3AWdExOr2ykXEnIioi4i62tra0gVoZlZi5ezNDGVMCpLGAz8H/joiXi5XHGZmebLltaQ38/DDe1jzkaQ7gJOAGkkrgMuAPgARcSPwTWAkcIMkgKaIqMsqHjOz7qD5zQJNVDF6cnlaRbK8++icDtZfCFyY1fHNzLqjqpX1NGg0Y/uXpyEnL3cfmZkZ0HdNgdV9y9N0BE4KZma5MqiMvZnBScHMLFdGbKlnS5l6M4OTgplZbsS27dQ0N9A0ys1HZmYVb+0fVwLQa5xrCmZmFW/1i0nHtb7vcVIwM6t4G15JOq6VqzczOCmYmeXG1jL3ZgYnBTOz3GhaXqAZMWryqLLF4KRgZpYTSW/mUfQfXL4HWDspmJnlRN81BVZXl6/pCJwUzMxyY+CGAm+XsTczOCmYmeXGiC31bBnmpGBmVvGiaQcjd6wqa29mcFIwM8uFdS+voopmtJ9rCmZmFW/NwrQ3c5nGZt4ps6Qg6WZJqyQtbGe9JH1f0lJJCyRNyyoWM7O8e/ulpOPawEN6bvPRrcCpu1k/EzgknWYBP8wwFjOzXGsZm3lSD60pRMRjwJrdFDkD+H+RmAsMk1Tev4aZWZnsWJ40H9VOHl3WOMp5TWF/YHnR/Ip02S4kzZI0T9K8hoaGkgRnZlZKeqtAo2oYOLy6rHGUMymojWXRVsGImBMRdRFRV1tbm3FYZmalV72mQGOZezNDeZPCCuCAovlxQH2ZYjEzK6tBb9fz9oDyt6CXMyncB/xNehfS+4H1EVEoYzxmZmUzbHOBzWXuzQyQ2aP4JN0BnATUSFoBXAb0AYiIG4EHgNOApcA7wPlZxWJmlmexo5maHStZVObezJBhUoiIczpYH8DfZnV8M7Pu4u1ljQylCY0tf03BPZrNzMps59jM1WXuzQxOCmZmZdfSm/ng8jcfOSmYmZXZ5mVJUhj6XtcUzMwqXlPam3nUEWPKHImTgplZ2emtAqsZweDafuUOxUnBzKzcqhsLrK4uf9MROCmYmZXdwJz0ZgYnBTOzshu2ucCmYeW/8wg6mRQkTZTUN319kqQvSxqWbWhmZhUggtqmAk013aumcA+wQ9LBwI+BA4GfZhaVmVmF2Pin1VSzHco8NvNOnU0KzRHRBHwCuDYivgrk4wzMzLqxhgVJH4U+4/PxldrZpLBd0jnA/wTuT5f1ySYkM7PKkafezND5pHA+cBxwVUS8JulA4CfZhWVmVhk2v5p0XMtDb2bo5FNSI2Ix8GUAScOBwRFxdZaBmZlVgu1vJDWFmin5SAqdvfvoUUlDJI0AXgBukfR/sw3NzKwCvFVgHUMZOnZAuSMBOt98NDQi3gb+CrglIo4GZnS0kaRTJb0kaamkS9tYP17SI5Kel7RA0ml7Fr6ZWfdW3Vigsc9Y1Nao9WXQ2aTQW9JY4FP8+ULzbkmqAq4HZgKTgHMkTWpV7BvAXRFxFHA2cEMn4zEz6xEGrq9nfU56M0Pnk8IVwIPAqxHxjKSDgFc62GY6sDQilkXENuBO4IxWZQIYkr4eCtR3Mh4zsx5h6OYC7wzNx51H0PkLzXcDdxfNLwPO7GCz/YHlRfMrgGNblbkceEjSl4CBtNMkJWkWMAtg/PjxnQnZzCz/IqjZXuDlnPRmhs5faB4n6V5JqyStlHSPpHEdbdbGsmg1fw5wa0SMA04D/l3SLjFFxJyIqIuIutra2s6EbGaWe5veXEd/tuRibOadOtt8dAtwH7AfSQ3gV+my3VkBHFA0P45dm4cuAO4CiIingH5ATSdjMjPr1hpfTG5H7T0+P81HnU0KtRFxS0Q0pdOtQEc/2Z8BDpF0oKRqkgvJ97Uq8wbwEQBJ7yNJCg2djt7MrBtb/8ckKQyY2P1qCo2SzpVUlU7nAqt3t0H6rKSLSC5QLyG5y2iRpCsknZ4W+zvg85JeAO4AzouI1k1MZmY90jtL89WbGTp5oRn4HHAd8D2S6wJPkjz6Yrci4gHggVbLvln0ejFwfGeDNTPrSXb2Zh45OT9JoVM1hYh4IyJOj4jaiBgVEX9J0pHNzMz2VqHABgYxfPzgckfSYl9GXruky6IwM6tAfRrraeydn97MsG9JIUenYWbW/QxYV2Bd//w0HcG+JQVfEDYz2wdD3ymwKUe9maGDC82SNtD2l7+A/plEZGZWCSIYub3AqyPzVVPYbVKIiPxc/TAz60E2r9rAIDYROerNDPvWfGRmZntpZ2/mPjnqzQxOCmZmZbFuSZIU+h/kmoKZWcXblPZmHnKYk4KZWcXb/qd0bOYj3HxkZlbxor7AO/RnxIQhHRcuIScFM7My6NNQT0PVWNQrX/2AnRTMzMqg//r89WYGJwUzs7IYsqnAxiH5up4ATgpmZmVRu62ebTnrzQwZJwVJp0p6SdJSSZe2U+ZTkhZLWiTpp1nGY2aWB1vXbGIwG4gx+UsKnR1kZ49JqgKuBz5KMl7zM5LuSwfW2VnmEODrwPERsVbSqKziMTPLi4YFBcYBVQdUVvPRdGBpRCyLiG3AncAZrcp8Hrg+ItYCRMSqDOMxM8uFdYuTjmt5680M2SaF/YHlRfMr0mXFDgUOlfSEpLmSTm1rR5JmSZonaV5DQ0NG4ZqZlcampUnHtbz1ZoZsk0JbN9+2fgx3b+AQ4CTgHOAmScN22ShiTkTURURdbW1tlwdqZlZK29LezCMmV1bz0QrggKL5cUB9G2V+GRHbI+I14CWSJGFm1mNFfYEt9KXmkOHlDmUXWSaFZ4BDJB0oqRo4G7ivVZlfACcDSKohaU5almFMZmZl13tVPQ1VY+hVla/ezJBhUoiIJuAi4EFgCXBXRCySdIWk09NiDwKrJS0GHgG+FhGrs4rJzCwP+q8rsLZf/pqOIMNbUgEi4gHggVbLvln0OoBL0snMrCIM2VRg5Yj3ljuMNrlHs5lZiY3cVs+2Efm78wicFMzMSmrb+s0Mi3U0j8ln85GTgplZCTUufAuAqnGuKZiZVby1i5I78/sd6KRgZlbx8tybGZwUzMxKauvr+e3NDE4KZmYlFW/Ws53e1Bw2styhtMlJwcyshKoaCjT0GkNVn3x+/eYzKjOzHqrf2gJrctqbGZwUzMxKavDGAhsH5/MiMzgpmJmV1Mit9WzJaW9mcFIwMyuZpne2MTJW0zzazUdmZhUv772ZwUnBzKxk1ixMejP3zWlvZnBSMDMrmY2vJB3XBh9aoc1Hkk6V9JKkpZIu3U25T0oKSXVZxmNmVk4tvZkPr8CagqQq4HpgJjAJOEfSpDbKDQa+DPwhq1jMzPKgeUU9O+hF7aTacofSrixrCtOBpRGxLCK2AXcCZ7RR7krgO8CWDGMxMyu7qlUFGnqNpnffqnKH0q4sk8L+wPKi+RXpshaSjgIOiIj7M4zDzCwX+q0tsKZvfq8nQLZJQW0si5aVUi/ge8DfdbgjaZakeZLmNTQ0dGGIZmalM3hjPRsG5fd6AmSbFFYABxTNjwPqi+YHA5OBRyW9DrwfuK+ti80RMSci6iKirrY2v21xZma7M2JrIde9mSHbpPAMcIikAyVVA2cD9+1cGRHrI6ImIiZExARgLnB6RMzLMCYzs7LYsWU7I5sb2DGqQpuPIqIJuAh4EFgC3BURiyRdIen0rI5rZpZHjYtW0oug1/75rin0znLnEfEA8ECrZd9sp+xJWcZiZlZOaxcXGE1+x2beyT2azcxKYMPLSce1QTnuzQxOCmZmJbHltSQpDJ/kmoKZWcVrXlFPM6J28uhyh7JbTgpmZiXQa2WBRo2iekCml3L3mZOCmVkJ9F1bYE3ffDcdgZOCmVlJDN5Qz9s5780MTgpmZiUxfEuBLcPzfecROCmYmWWuefsOaptXsmOUawpmZhVv9ZJVVNGM9nNSMDOreGsWJX0U+k5wUjAzq3jdpTczOCmYmWVuy7Jk1IBh73NNwcys4u1YkdQURh0xpsyRdMxJwcwsY0lv5hr6Dq4udygdclIwM8tY39X1rKnOf9MROCmYmWVu4IZCt+jNDBknBUmnSnpJ0lJJl7ax/hJJiyUtkPRbSe/JMh4zs3IYvqXA5mH5v/MIMkwKkqqA64GZwCTgHEmTWhV7HqiLiCOAnwHfySoeM7NyiB3N1O54i6Za1xSmA0sjYllEbAPuBM4oLhARj0TEO+nsXGBchvGYmZXcmpcb6UMTyvnYzDtlmRT2B5YXza9Il7XnAuDXba2QNEvSPEnzGhoaujBEM7NsrV6Y3I5a/Z4Kbz4C1MayaLOgdC5QB3y3rfURMSci6iKirra2tgtDNDPLVktv5kO6R00hyyGAVgAHFM2PA+pbF5I0A/gn4MSI2JphPGZmJbf51e7TmxmyrSk8Axwi6UBJ1cDZwH3FBSQdBfwIOD0iVmUYi5lZWTQtT2oKtUdUeFKIiCbgIuBBYAlwV0QsknSFpNPTYt8FBgF3S5ov6b52dmdm1i31WllgrYbTf3i/cofSKZmOIB0RDwAPtFr2zaLXM7I8vplZuVWvrmd1n7EML3cgneQezWZmGRr0doH1A7tH0xE4KZiZZWrY5u7TmxmcFMzMMhPNQe2OAtu7SW9mcFIwM8vMumVr6Ms2NNZJwcys4rX0Zp7g5iMzs4q3/o9JUhgw0TUFM7OKtzkdm3n4JCcFM7OKt7M3c80UJwUzs4qnQoH1DGHgqIHlDqXTnBTMzDJSvbqe1d1kbOadnBTMzDIycH2B9QOcFMzMDBi6ucA7Q7vP7ajgpGBmloloDkY11bO9xjUFM7OK9/by9fRnC3Sj3szgpGBmlonGF7tfb2bIOClIOlXSS5KWSrq0jfV9Jf1Huv4PkiZkFcvK+QXmDzuRVQveyuoQthcq6X3pLueahzhLGUNWx1r12xcBUL++XbrfrGWWFCRVAdcDM4FJwDmSJrUqdgGwNiIOBr4HXJNVPH/3wBzjAAAJaklEQVQ890qmrH+cJZ+5IqtD2F6opPelu5xrHuIsZQxZHav6th8BsOOuu7t0v1lTRGSzY+k44PKI+Fg6/3WAiPh2UZkH0zJPSeoNvAXUxm6Cem/fkXHTuL/odBzvX/ZTerNjl+VNVDH3oM90ej/WtSrpfeku55qHOEsZQ1bHam+/m+lH/9i81/vdV5KejYi6DstlmBQ+CZwaERem838NHBsRFxWVWZiWWZHOv5qWaWy1r1nALIApVB99f+/9Ox1Hr+YmhjWvYQDv0IugGfEOA1jXawTNvTIdjdR2o5Lel+5yrnmIs5QxZHWs1vt9hwE8P+ETHPLLf2HUEWO68Az2TGeTAhGRyQScBdxUNP/XwA9alVkEjCuafxUYubv9Hn300bGnfjdpdjTRK96hXzTRKx49/It7vA/repX0vnSXc81DnKWMIatj5eHv2BowLzrx3Z3lheYVwAFF8+OA+vbKpM1HQ4E1XR1In7Urefzw2bzxH3N5/PDZVK/J94W+SlFJ70t3Odc8xFnKGLI6Vh7+jnsry+aj3sDLwEeAN4FngM9ExKKiMn8LTImI2ZLOBv4qIj61u/3W1dXFvHnzMonZzKyn6mzzUWYNhRHRJOki4EGgCrg5IhZJuoKkGnMf8GPg3yUtJakhnJ1VPGZm1rFMrx5FxAPAA62WfbPo9RaSaw9mZpYD7tFsZmYtnBTMzKyFk4KZmbVwUjAzsxaZ3ZKaFUkNwJ9aLR4KrM/okF25767a177spwZo7LCUlVqWn+G86S7nmoc4uzKG90REbUeFul1SaIukORExK+/77qp97ct+JM3rzL3KVlpZfobzprucax7iLEcMPaX56FfdZN9dta8sz9fKo5Le0+5yrnmIs+Qx9IiagnWeawpmtjs9paZgnTen3AGYWX65pmBmZi1cUzAzsxZOCmZm1sJJISXpLyX9m6RfSjql3PGY7QlJB0n6saSflTuWrHWXc+0ucbZWtqQgaZikn0n6o6Ql6ZjOe7OfmyWtSof2bL3uVEkvSVoq6dLd7ScifhERnwfOAz69N7F0d931Q1xqkg6Q9Ej6uV0k6Sv7sK+u+vwui4gL9jaO3cTXT9LTkl5Iz/Vb+7CvzM9VUpWk5yXdn+c4c60zw7NlMQG3ARemr6uBYa3WjwIGt1p2cBv7OQGYBixstbyKZHjPg9L9vwBMAqYA97eaRhVt93+AaeX6u+zD3/NmYFUbf4dTgZeApcClndzXz8p9PnmegLE7PyPAYJLBpCa1KlOuz2+XvneAgEHp6z7AH4D35/VcgUuAnwL3t7EuN3HmeSrXf6ohwGukdz+1U+Ys4L+Bfun854EH2ik7oY038DjgwaL5rwNf383xBFwDzCj3m7KXf9NdPsiV8iEu9wT8Evhoq2Ul/fyW4r0DBgDPAcfm8VxJhvz9LfDhdpJCLuLM+1Su5qODgAbglrSqd5OkgcUFIuJu4L+AOyV9FvgcsNuhOlvZH1heNL8iXdaeLwEzgE9Kmr0Hx8mFiHiMXce3ng4sjaQauw24EzgjIl6MiI+3mlaVPOgeQNIE4CiSX9AtSv35lTRS0o3AUZK+vgfH6VDaJDOfpCb6m4jI67leC/w90NzWdjmKM9fKlRR6k/yq/WFEHAVsAnZpn4uI7wBbgB8Cp0fExj04htpY1m6njIj4fkQcHRGzI+LGPThOnlXEh7hcJA0C7gEujoi3W68v8ed3dfrZnRgR396D43QoInZExFSSX+LTJU1uo0xZz1XSx4FVEfFsB+fSI96TLJUrKawAVhT94vgZSZJ4F0kfAiYD9wKX7cUxDiiaHwfU73mo3VpFfIjLQVIfkoRwe0T8vJ0yPerzGxHrgEdJrlO9Sw7O9XjgdEmvk9SIPyzpJzmMM/fKkhQi4i1guaTD0kUfARYXl5F0FPBvwBnA+cAISf+8B4d5BjhE0oGSqoGzgfv2OfjupSI+xKUmScCPgSUR8X/bKdMjPr+SaiUNS1/3J2li/WOrMmU/14j4ekSMi4gJ6fb/HRHn5i3ObqFcFzOAqcA8YAHwC2B4q/XHA1OK5vsAn29jP3cABWA7yZfgBUXrTiO5M+RV4J/KefGmRH/TCbz7QnNvYBlwIH++0Hx4uePs7hPwQZIa1wJgfjqd1qpMj/j8AkcAz6fnuhD4ZhtlcnWuwEm0faE5V3HmdfKzj3oISXeQ/GeoAVYCl0XEjyWdRnIBrgq4OSKuKl+UZpZ3TgpmZtbCj7kwM7MWTgpmZtbCScHMzFo4KZiZWQsnBTMza+GkYGZmLZwUrMeQtCfPsemK490kaVKJj3mxpAGlPKZVFvdTsB5D0saIGNSF++sdEU1dtb9OHlMk/y/bfNJn+myfuohoLGVcVjlcU7AeLX12zz2Snkmn49Pl0yU9mT66/cmdz+GSdJ6kuyX9CnhI0kmSHtWfRwm8Pf3iJl1el77eKOmqdISyuZJGp8snpvPPSLqirdqMpAlKRnG7gWS8ggMk/VDSvOLRziR9GdgPeETSI+myUyQ9Jem5NO4uS4pWocr9nA1PnrpqAja2seynwAfT1+NJHmIHyUBPvdPXM4B70tfnkTzvZkQ6fxKwnuRhgr2Ap4r29yjJr3ZInoX0P9LX3wG+kb6+HzgnfT27nRgnkIwB8P6iZTuPX5Ue54h0/nWgJn1dAzwGDEzn/4E2nk3kydOeTL33NamY5dwMYFL64x5giKTBwFDgNkmHkHyh9yna5jcRUTxg0dMRsQIgHWxmAvB4q+NsI0kAAM8CH01fHwf8Zfr6p8C/tBPnnyJibtH8pyTNInmo4ViSEfMWtNrm/enyJ9LzqyZJWmZ7zUnBerpewHERsbl4oaQfAI9ExCfS0dMeLVq9qdU+tha93kHb/2+2R0R0UGZ3Wo4p6UDgfwPHRMRaSbcC/drYRiQJ7Jw9PJZZu3xNwXq6h4CLds5Impq+HAq8mb4+L8PjzwXOTF+f3clthpAkifXptYmZRes2AIOL9n28pIMBJA2QdOi+h2yVzEnBepIBklYUTZcAXwbqJC2QtJikXR+Sdv9vS3qCpN0+KxcDl0h6mqQZaH1HG0TECyRjGCwCbgaeKFo9B/i1pEciooEkod0haQFJknhv14Zvlca3pJplKO1TsDkiQtLZJBedzyh3XGbt8TUFs2wdDVyX3sa6DvhcmeMx2y3XFMzMrIWvKZiZWQsnBTMza+GkYGZmLZwUzMyshZOCmZm1cFIwM7MW/x+RgAeMwvztawAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2929df7bcf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Schéma moche parce que nb_iter bien trop faible\n",
    "plot_train_test(train_gd_mean, valid_gd_mean, gammas, \"Least squares GD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0642857142857143\n",
      "0.9706352985392875\n"
     ]
    }
   ],
   "source": [
    "# Minimum values for ls_gd\n",
    "idx = np.argmin(valid_gd_mean)\n",
    "learning_rate = gammas[idx]\n",
    "ls_gd_loss = np.min(valid_gd_mean)\n",
    "print(learning_rate)\n",
    "print(ls_gd_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_sgd_hyperparam(gammas, nb_cross_validation, x_train, y_train):\n",
    "    loss_valid = np.zeros([len(gammas), nb_cross_validation])\n",
    "    loss_train = np.zeros([len(gammas), nb_cross_validation])\n",
    "    \n",
    "    nb_elem = math.floor(x_train.shape[0]/nb_cross_validation)\n",
    "    \n",
    "    for i, gamma in enumerate(gammas):\n",
    "        for k in range(nb_cross_validation):\n",
    "            x_valid_k = x_train[k*nb_elem:(k+1)*nb_elem][:]  \n",
    "            y_valid_k = y_train[k*nb_elem:(k+1)*nb_elem]\n",
    "            \n",
    "            x_train_k = np.concatenate([x_train[0:k*nb_elem][:], x_train[(k+1)*nb_elem:][:]])\n",
    "            y_train_k = np.concatenate([y_train[0:k*nb_elem],    y_train[(k+1)*nb_elem:]   ]) \n",
    "                                        \n",
    "            w, loss_tr = least_squares_SGD(y_train_k, x_train_k, w_initial, max_iters, gamma)\n",
    "            loss_train[i][k] = loss_tr\n",
    "            loss_valid[i][k] = compute_loss_ls(y_valid_k, x_valid_k, w)\n",
    "            \n",
    "    return loss_valid, loss_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b205ca0700c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss_test_sgd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_train_sgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mls_sgd_hyperparam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgammas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_cross_validation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-04d188acbf09>\u001b[0m in \u001b[0;36mls_sgd_hyperparam\u001b[1;34m(gammas, nb_cross_validation)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgammas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_cross_validation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleast_squares_SGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_initial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[0mloss_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_tr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mloss_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loss_ls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\MLProjet\\implementations.py\u001b[0m in \u001b[0;36mleast_squares_SGD\u001b[1;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m             \u001b[1;31m# compute a stochastic gradient and loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_gradient_least_squares\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\MLProjet\\implementations.py\u001b[0m in \u001b[0;36mbatch_iter\u001b[1;34m(y, tx, batch_size, num_batches, shuffle)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mshuffle_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m         \u001b[0mshuffled_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[0mshuffled_tx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_valid_sgd, loss_train_sgd = ls_sgd_hyperparam(gammas, nb_cross_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sgd_mean = np.mean(loss_train_sgd, axis=1)\n",
    "valid_sgd_mean = np.mean(loss_valid_sgd, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_test(train_sgd_mean, valid_sgd_mean, gammas, \"Least squares SGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum values for ls_sgd\n",
    "idx = np.argmin(valid_sgd_mean)\n",
    "learning_rate = gammas[idx]\n",
    "ls_sgd_loss = np.min(valid_sgd_mean)\n",
    "print(learning_rate)\n",
    "print(ls_sgd_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_sgd_hyperparam(gammas, lambdas, nb_cross_validation, x_train, y_train):\n",
    "    loss_train = np.zeros([len(gammas), len(lambdas), nb_cross_validation])\n",
    "    loss_valid = np.zeros([len(gammas), len(lambdas), nb_cross_validation])\n",
    "    \n",
    "    nb_elem = math.floor(x_train.shape[0]/nb_cross_validation)\n",
    "    \n",
    "    for i, gamma in enumerate(gammas):\n",
    "        for j, lambda_ in enumerate(lambdas):\n",
    "            for k in range(nb_cross_validation):\n",
    "                x_valid_k = x_train[k*nb_elem:(k+1)*nb_elem][:]  \n",
    "                y_valid_k = y_train[k*nb_elem:(k+1)*nb_elem]\n",
    "\n",
    "                x_train_k = np.concatenate([x_train[0:k*nb_elem][:], x_train[(k+1)*nb_elem:][:]])\n",
    "                y_train_k = np.concatenate([y_train[0:k*nb_elem],    y_train[(k+1)*nb_elem:]   ]) \n",
    "\n",
    "                w, loss_gamma = ridge_SGD(y_train_k, x_train_k, w_initial, max_iters, gamma, lambda_)\n",
    "                loss_train[i][j][k] = loss_gamma\n",
    "                loss_test[i][j][k] = compute_loss_ridge(y_valid_k, x_valid_k, w, lambda_)\n",
    "                \n",
    "    return loss_train, loss_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_valid_r_sgd, loss_train_r_sgd = ridge_sgd_hyperparam(gammas, lambdas, nb_cross_validation, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sgd_mean = np.mean(loss_train_r_sgd, axis=2)\n",
    "valid_sgd_mean = np.mean(loss_valid_r_sgd, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot 2D à coder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum values for ls_sgd\n",
    "ind = np.unravel_index(np.argmin(valid_sgd_mean, axis=None), valid_sgd_mean.shape)\n",
    "learning_rate = gammas[ind[0]]\n",
    "ls_sgd_loss = valid_sgd_mean[ind]\n",
    "print(learning_rate)\n",
    "print(ls_sgd_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GD Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_gd_hyperparam(gammas, lambdas, nb_cross_validation, x_train, y_train):\n",
    "    loss_train = np.zeros([len(gammas), len(lambdas), nb_cross_validation])\n",
    "    loss_valid = np.zeros([len(gammas), len(lambdas), nb_cross_validation])\n",
    "    \n",
    "    nb_elem = math.floor(x_train.shape[0]/nb_cross_validation)\n",
    "    \n",
    "    for i, gamma in enumerate(gammas):\n",
    "        for j, lambda_ in enumerate(lambdas):\n",
    "            for k in range(nb_cross_validation):\n",
    "                \n",
    "                x_valid_k = x_train[k*nb_elem:(k+1)*nb_elem][:]  \n",
    "                y_valid_k = y_train[k*nb_elem:(k+1)*nb_elem]\n",
    "\n",
    "                x_train_k = np.concatenate([x_train[0:k*nb_elem][:], x_train[(k+1)*nb_elem:][:]])\n",
    "                y_train_k = np.concatenate([y_train[0:k*nb_elem],    y_train[(k+1)*nb_elem:]   ]) \n",
    "\n",
    "                w, loss_gamma = ridge_SGD(y_train_k, x_train_k, w_initial, max_iters, gamma, lambda_)\n",
    "                loss_train[i][j][k] = loss_gamma\n",
    "                loss_test[i][j][k] = compute_loss_ridge(y_valid_k, x_valid_k, w, lambda_)\n",
    "                \n",
    "    return loss_train, loss_valid_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_valid_r_gd, loss_train_r_gd = ridge_gd_hyperparam(gammas, lambdas, nb_cross_validation, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gd_mean = np.mean(loss_train_r_gd, axis=2)\n",
    "valid_gd_mean = np.mean(loss_valid_r_gd, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO plot 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum values for ls_sgd\n",
    "ind = np.unravel_index(np.argmin(valid_gd_mean, axis=None), valid_gd_mean.shape)\n",
    "learning_rate = gammas[ind[0]]\n",
    "ls_gd_loss = valid_gd_mean[ind]\n",
    "print(learning_rate)\n",
    "print(ls_gd_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pas encore en fonction car pas sûre. Voir correction lab05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "Debugged but not sure from here: Il y aura probablement une correction du labo 5 pour améliorer / vérifier les fonctions de logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Il y a un NaN dans la loss à cause d'un outlier. Je sais pas si on choisis d'enlever ou comment traiter. On verra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99981039 0.90820056 0.92786083 ... 0.28142199 0.00709906 0.04907172]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(tx, w):\n",
    "    \"\"\"Compute sigmoid function\"\"\"\n",
    "    z = np.array(np.exp(-tx.dot(w)))\n",
    "    return 1./(1 + z)\n",
    "\n",
    "value = sigmoid(x_train, w_initial)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logreg_loss(y, tx, w):  #np.log parce que math.log fonctionne pas.. J'ai toujours pas compris pourquoi.\n",
    "    \"\"\"Compute error and gradient of logistic regression\"\"\"\n",
    "    sig = sigmoid(tx, w)\n",
    "    loss = np.sum((-y * np.log(sig) - (1-y) * np.log(1-sig)), axis = -1)/len(y)\n",
    "    print(max(sig))\n",
    "    #a = (-y * np.log(sig) - (1-y) * np.log(1-sig))\n",
    "    #print(max(a))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.51459227 0.10073464 0.53488351 0.42669814 0.02003378 0.140064\n",
      " 0.2264286  0.072293   0.49416143 0.6004996  0.14155126 0.00350222\n",
      " 0.81835964 0.2093738  0.7436304  0.88667924 0.88977018 0.11193385\n",
      " 0.70287969 0.70028424 0.34716633 0.66417114 0.94782936 0.73171242\n",
      " 0.5700489  0.53273724 0.82729083 0.43374625 0.09599914 0.22535177\n",
      " 0.38780099]\n",
      "1.0\n",
      "loss is nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pauli\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Pauli\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in multiply\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "loss = compute_logreg_loss(y_train, x_train, w_initial)\n",
    "print(\"loss is {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logreg_grad(y, tx, w):\n",
    "    \"\"\"Compute error and gradient of logistic regression\"\"\" \n",
    "    sig = sigmoid(tx, w)\n",
    "    err  = sig - y\n",
    "    grad = tx.T.dot(err)/len(y)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_lr = compute_logreg_grad(y_train, x_train, w_initial)\n",
    "print(\"grad is {} \\n of shape {}\".format(grad_lr, grad_lr.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.01\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma): #SGD  (GD easy to implement from here)\n",
    "    \"\"\"Stochastic Gradient Descent algorithm with logistic regression.\"\"\"\n",
    "    \"\"\"Required by project description\"\"\"\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient\n",
    "            grad = compute_logreg_grad(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # compute a stochastic loss\n",
    "            compute_logreg_loss(y_batch, tx_batch, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_lr, loss_lr = logistic_regression(y_train, x_train, w_initial, max_iters, gamma)\n",
    "print(\"w is {} \\n of shape {}\".format(w_lr, w_lr.shape))\n",
    "print(\"\\n loss of ridge_SGD is {}\".format(loss_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logreg_reg_loss(y, tx, w, lambda_):\n",
    "    \"\"\"Compute error and gradient of logistic regression\"\"\"\n",
    "    reg = ( lambda_/(2*len(y)) ) * sum(w**2)\n",
    "    loss = compute_logreg_loss(y, tx, w) + reg\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_lrr = compute_logreg_reg_loss(y_train, x_train, w_initial, lambda_)\n",
    "print(\"loss is {}\".format(loss_lrr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logreg_reg_grad(y, tx, w, lambda_):\n",
    "    \"\"\"Compute error and gradient of logistic regression\"\"\"     \n",
    "    grad = compute_logreg_grad(y, tx, w) \n",
    "    reg = (lambda_/len(y)) * w[1:]\n",
    "    grad[1:] = grad[1:] + reg            \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_lrr = compute_logreg_reg_grad(y_train, x_train, w_initial, lambda_)\n",
    "print(\"grad is {} \\n of shape {}\".format(grad_lrr, grad_lrr.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# La loss de ridge SGD me semble particulièrement haute. A vérifier avec les plots (et/ou code).\n",
    "gamma = 0.01\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    \"\"\"Stochastic Gradient Descent algorithm with REGULARIZED logistic regression.\"\"\"\n",
    "    \"\"\"Required by project description\"\"\"\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient\n",
    "            grad = compute_logreg_reg_grad(y_batch, tx_batch, w, lambda_)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # compute a stochastic loss\n",
    "            loss = compute_logreg_reg_loss(y_batch, tx_batch, w, lambda_)\n",
    "    return w_rlr_sgd, loss_rlr_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_rlr_sgd, loss_rlr_sgd = reg_logistic_regression(y_train, x_train, lambda_, w_initial, max_iters, gamma)\n",
    "print(\"w is {} \\n of shape {}\".format(w_rlr_sgd, w_rlr_sgd.shape))\n",
    "print(\"\\n loss of ridge_SGD is {}\".format(loss_rlr_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction de chaque méthode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_labels(w, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
