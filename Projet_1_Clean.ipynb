{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import load_csv_data, predict_labels, create_csv_submission\n",
    "from implementations import least_squares_GD, least_squares_SGD, least_squares, ridge_regression, batch_iter, ridge_GD, ridge_SGD\n",
    "from preprocessing import standardize_train, standardize_test, add_bias\n",
    "from losses_gradients import compute_loss_ls, compute_gradient_least_squares, compute_loss_ridge, compute_gradient_ridge\n",
    "from plots import plot_train_test\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, input_train, ids_train = load_csv_data('train.csv', sub_sample=False)\n",
    "y_test, input_test, ids_test = load_csv_data('test.csv', sub_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEE: on pourrait ausi tenter build_polynomial comme dans les séries si on est motivés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise data\n",
    "# Careful to standardize the x_test with the mean and std of x_train\n",
    "x_train, mean, std = standardize_train(input_train)\n",
    "x_train = add_bias(x_train)\n",
    "x_test = standardize_test(input_test, mean, std)\n",
    "x_test = add_bias(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods to implement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm. (probably change afterwards)\n",
    "max_iters = 200                                          \n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.random.rand(x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For grid search of hyperparameters\n",
    "num_intervals = 25\n",
    "gammas = np.linspace(0, 0.90, num_intervals)\n",
    "lambdas = np.logspace(-4, -0.05, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least square "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_gd_hyperparam(gammas):\n",
    "    loss_test = []\n",
    "    loss_train = []\n",
    "    for gamma in gammas:\n",
    "        w, loss_tr = least_squares_GD(y_train, x_train, w_initial, max_iters, gamma)\n",
    "        loss_train.append(loss_tr)\n",
    "        loss_test = compute_loss_ls(y_test, x_test, w)\n",
    "    return loss_test, loss_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_sgd_hyperparam(gammas):\n",
    "    loss_test = []\n",
    "    loss_train = []\n",
    "    for gamma in gammas:\n",
    "        w, loss_tr = least_squares_SGD(y_train, x_train, w_initial, max_iters, gamma)\n",
    "        loss_train.append(loss_tr)\n",
    "        loss_test = compute_loss_ls(y_test, x_test, w)\n",
    "    return loss_test, loss_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test_gd, loss_train_gd = ls_gd_hyperparam(gammas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_test(loss_train_gd, loss_test_gd, gammas, \"Least squares GD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum values for ls_gd\n",
    "idx = np.argmin(loss_test_gd)\n",
    "learning_rate = gammas[idx]\n",
    "ls_gd_loss = min(loss_test_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test_sgd, loss_train_sgd = ls_sgd_hyperparam(gammas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_test(loss_train_sgd, loss_test_sgd, gammas, \"Least squares SGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum values for ls_sgd\n",
    "idx = np.argmin(loss_test_sgd)\n",
    "learning_rate = gammas[idx]\n",
    "ls_sgd_loss = min(loss_test_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_sgd_hyperparam(gammas, lambdas):\n",
    "    loss_train = np.zeros((len(gammas), len(lamdas)))\n",
    "    loss_test = np.zeros((len(gammas), len(lamdas)))\n",
    "    for i, gamma in enumerate(gammas):\n",
    "        for j, lambda_ in enumerate(lambdas):\n",
    "            w, loss_gamma = ridge_SGD(y_train, x_train, w_initial, max_iters, gamma, lambda_)\n",
    "            loss_train[i, j] = loss_gamma\n",
    "            loss_test[i, j] = compute_loss_ridge(y_test, x_test, w, lambda_)\n",
    "    #min_gamma, min_lambda = np.unravel_index(np.argmin(loss_lamb), loss_lamb.shape)\n",
    "    return loss_train, loss_test #loss_lamb[min_gamma, min_lambda], gammas[min_gamma], lambdas[min_lambda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_gd_hyperparam(gammas, lambdas):\n",
    "    loss_train = np.zeros((len(gammas), len(lamdas)))\n",
    "    loss_test = np.zeros((len(gammas), len(lamdas)))\n",
    "    for i, gamma in enumerate(gammas):\n",
    "        for j, lambda_ in enumerate(lambdas):\n",
    "            w, loss_gamma = ridge_GD(y_train, x_train, w_initial, max_iters, gamma, lambda_)\n",
    "            loss_train[i, j] = loss_gamma\n",
    "            loss_test[i, j] = compute_loss_ridge(y_test, x_test, w, lambda_)\n",
    "    return loss_train, loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test_gd, loss_train_gd = ridge_gd_hyperparam(gammas, lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO plot 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum values for ridge_gd\n",
    "#(i,j) = np.argmin(loss_test_gd)\n",
    "#learning_rate = gammas[i]\n",
    "#best_lambda = lambdas[j]\n",
    "ridge_gd_loss = min(loss_test_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test_sgd, loss_train_sgd = ridge_sgd_hyperparam(gammas, lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "Debugged but not sure from here: Il y aura probablement une correction du labo 5 pour améliorer / vérifier les fonctions de logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Il y a un NaN dans la loss à cause d'un outlier. Je sais pas si on choisis d'enlever ou comment traiter. On verra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99981039 0.90820056 0.92786083 ... 0.28142199 0.00709906 0.04907172]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(tx, w):\n",
    "    \"\"\"Compute sigmoid function\"\"\"\n",
    "    z = np.array(np.exp(-tx.dot(w)))\n",
    "    return 1./(1 + z)\n",
    "\n",
    "value = sigmoid(x_train, w_initial)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logreg_loss(y, tx, w):  #np.log parce que math.log fonctionne pas.. J'ai toujours pas compris pourquoi.\n",
    "    \"\"\"Compute error and gradient of logistic regression\"\"\"\n",
    "    sig = sigmoid(tx, w)\n",
    "    loss = np.sum((-y * np.log(sig) - (1-y) * np.log(1-sig)), axis = -1)/len(y)\n",
    "    print(max(sig))\n",
    "    #a = (-y * np.log(sig) - (1-y) * np.log(1-sig))\n",
    "    #print(max(a))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.51459227 0.10073464 0.53488351 0.42669814 0.02003378 0.140064\n",
      " 0.2264286  0.072293   0.49416143 0.6004996  0.14155126 0.00350222\n",
      " 0.81835964 0.2093738  0.7436304  0.88667924 0.88977018 0.11193385\n",
      " 0.70287969 0.70028424 0.34716633 0.66417114 0.94782936 0.73171242\n",
      " 0.5700489  0.53273724 0.82729083 0.43374625 0.09599914 0.22535177\n",
      " 0.38780099]\n",
      "1.0\n",
      "loss is nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pauli\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Pauli\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in multiply\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "loss = compute_logreg_loss(y_train, x_train, w_initial)\n",
    "print(\"loss is {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logreg_grad(y, tx, w):\n",
    "    \"\"\"Compute error and gradient of logistic regression\"\"\" \n",
    "    sig = sigmoid(tx, w)\n",
    "    err  = sig - y\n",
    "    grad = tx.T.dot(err)/len(y)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_lr = compute_logreg_grad(y_train, x_train, w_initial)\n",
    "print(\"grad is {} \\n of shape {}\".format(grad_lr, grad_lr.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.01\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma): #SGD  (GD easy to implement from here)\n",
    "    \"\"\"Stochastic Gradient Descent algorithm with logistic regression.\"\"\"\n",
    "    \"\"\"Required by project description\"\"\"\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient\n",
    "            grad = compute_logreg_grad(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # compute a stochastic loss\n",
    "            compute_logreg_loss(y_batch, tx_batch, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_lr, loss_lr = logistic_regression(y_train, x_train, w_initial, max_iters, gamma)\n",
    "print(\"w is {} \\n of shape {}\".format(w_lr, w_lr.shape))\n",
    "print(\"\\n loss of ridge_SGD is {}\".format(loss_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logreg_reg_loss(y, tx, w, lambda_):\n",
    "    \"\"\"Compute error and gradient of logistic regression\"\"\"\n",
    "    reg = ( lambda_/(2*len(y)) ) * sum(w**2)\n",
    "    loss = compute_logreg_loss(y, tx, w) + reg\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_lrr = compute_logreg_reg_loss(y_train, x_train, w_initial, lambda_)\n",
    "print(\"loss is {}\".format(loss_lrr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logreg_reg_grad(y, tx, w, lambda_):\n",
    "    \"\"\"Compute error and gradient of logistic regression\"\"\"     \n",
    "    grad = compute_logreg_grad(y, tx, w) \n",
    "    reg = (lambda_/len(y)) * w[1:]\n",
    "    grad[1:] = grad[1:] + reg            \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_lrr = compute_logreg_reg_grad(y_train, x_train, w_initial, lambda_)\n",
    "print(\"grad is {} \\n of shape {}\".format(grad_lrr, grad_lrr.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# La loss de ridge SGD me semble particulièrement haute. A vérifier avec les plots (et/ou code).\n",
    "gamma = 0.01\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    \"\"\"Stochastic Gradient Descent algorithm with REGULARIZED logistic regression.\"\"\"\n",
    "    \"\"\"Required by project description\"\"\"\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient\n",
    "            grad = compute_logreg_reg_grad(y_batch, tx_batch, w, lambda_)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # compute a stochastic loss\n",
    "            loss = compute_logreg_reg_loss(y_batch, tx_batch, w, lambda_)\n",
    "    return w_rlr_sgd, loss_rlr_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_rlr_sgd, loss_rlr_sgd = reg_logistic_regression(y_train, x_train, lambda_, w_initial, max_iters, gamma)\n",
    "print(\"w is {} \\n of shape {}\".format(w_rlr_sgd, w_rlr_sgd.shape))\n",
    "print(\"\\n loss of ridge_SGD is {}\".format(loss_rlr_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choisir méthode:\n",
    "(D'après moi)\n",
    "- Pour chaque méthode: faire un plot de test_error (=loss) pour un grand nombre d'iteration (assez pour que on voit que l'erreur commence à remonter si possible).  Il faudrait idealement voir la courbe descendre et monter dans le graphe (underfit à overfit)\n",
    "- Garder la valeur de n iter ou la test error était minimale (au cas où: risque d'être très long pour ridge si lambda_ trop grand)\n",
    "- Comparer les loss de chaque méthode à leur n_iter optimale\n",
    "- La meilleure est celle ou c'est le plus faible\n",
    "\n",
    "En plus comme ça on aura des beau plots et des arguments pour le rapport."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction de chaque méthode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = predict_labels(weights, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
