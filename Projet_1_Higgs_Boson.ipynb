{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import load_csv_data, predict_labels, create_csv_submission\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "(250000, 30)\n"
     ]
    }
   ],
   "source": [
    "y_train, input_train, ids_train = load_csv_data('train.csv', sub_sample=False)\n",
    "print(y_train.shape)\n",
    "print(input_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 138.47 ,   51.655,   97.827, ...,    1.24 ,   -2.475,  113.497],\n",
       "       [ 160.937,   68.768,  103.235, ..., -999.   , -999.   ,   46.226],\n",
       "       [-999.   ,  162.172,  125.953, ..., -999.   , -999.   ,   44.251],\n",
       "       ...,\n",
       "       [ 105.457,   60.526,   75.839, ..., -999.   , -999.   ,   41.992],\n",
       "       [  94.951,   19.362,   68.812, ..., -999.   , -999.   ,    0.   ],\n",
       "       [-999.   ,   72.756,   70.831, ..., -999.   , -999.   ,    0.   ]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_test, input_test, ids_test = load_csv_data('test.csv', sub_sample=False)\n",
    "print(y_test.shape)\n",
    "print(input_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_train(x):\n",
    "    ''' standardize training set\n",
    "    '''\n",
    "    centered_data = x - np.mean(x, axis=0)\n",
    "    std_data = centered_data / np.std(x, axis=0)\n",
    "    \n",
    "    return std_data, np.mean(x, axis=0), np.std(x, axis=0)\n",
    "\n",
    "def standardize_test(x, mean, std):\n",
    "    ''' standardize test set with same values as training set\n",
    "    '''\n",
    "    return (x-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_one_column(x):\n",
    "    vect_one = np.ones([x.shape[0],1])\n",
    "    return np.concatenate((vect_one, x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEE: on pourrait ausi tenter build_polynomial comme dans les séries si on est motivés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise data\n",
    "# Careful to standardize the x_test with the mean and std of x_train\n",
    "x_train, mean, std = standardize_train(input_train)\n",
    "x_train = add_one_column(x_train)\n",
    "#x_test = standardize_test(input_test, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 31)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "#print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.46141372,  0.06833197, ...,  1.5668    ,\n",
       "         1.55858439,  0.4125105 ],\n",
       "       [ 1.        ,  0.51670419,  0.55250482, ..., -0.63936657,\n",
       "        -0.63936694, -0.27381996],\n",
       "       [ 1.        , -2.33785898,  3.19515553, ..., -0.63936657,\n",
       "        -0.63936694, -0.29396985],\n",
       "       ...,\n",
       "       [ 1.        ,  0.38016991,  0.31931645, ..., -0.63936657,\n",
       "        -0.63936694, -0.31701723],\n",
       "       [ 1.        ,  0.35431502, -0.84532397, ..., -0.63936657,\n",
       "        -0.63936694, -0.74543941],\n",
       "       [ 1.        , -2.33785898,  0.66533608, ..., -0.63936657,\n",
       "        -0.63936694, -0.74543941]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE method GD and SGD from lab02\n",
    "I put it here to use as reference for the function to implement.\n",
    "\n",
    "Could try it too  --> will be less good than Ridge if find good lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm. (probably change afterwards)\n",
    "max_iters = 5                                                   \n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.ones(x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31,)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From lab02 correction\n",
    "# MSE GD and SGD\n",
    "def calculate_mse(e):\n",
    "    \"\"\"Calculate the mse for vector e.\"\"\"\n",
    "    return np.mean(e**2)/2\n",
    "\n",
    "def compute_loss_mse(y, tx, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "\n",
    "    You can calculate the loss using mse or mae.\n",
    "    \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    return calculate_mse(e)\n",
    "\n",
    "def compute_gradient_mse(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    err = y - tx.dot(w)\n",
    "    #print(err)\n",
    "    #print(len(err))\n",
    "    #print(-tx.T.dot(err))\n",
    "    grad = -tx.T.dot(err) / len(err)\n",
    "    return grad, err\n",
    "\n",
    "def gradient_descent_mse(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute loss, gradient\n",
    "        grad, err = compute_gradient_mse(y, tx, w)\n",
    "        loss = calculate_mse(err)\n",
    "        # gradient w by descent update\n",
    "        w = w - gamma * grad\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws\n",
    "\n",
    "def stochastic_gradient_descent(\n",
    "        y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad, _ = compute_gradient_mse(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "            loss = compute_loss_mse(y, tx, w)\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        print(\"SGD({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/4): loss=112.59444572590787, w0=0.07973519999997924, w1=-1.814929860244586\n",
      "Gradient Descent(1/4): loss=5871.509948405188, w0=-0.19634424000014766, w1=16.00421250811494\n",
      "Gradient Descent(2/4): loss=337846.15328975685, w0=-0.2791680719989442, w1=-119.21362055336922\n",
      "Gradient Descent(3/4): loss=19447679.922442727, w0=-0.30401522160813466, w1=906.4767679639552\n",
      "Gradient Descent(4/4): loss=1119484055.6777604, w0=-0.31146936641814404, w1=-6876.383695387108\n"
     ]
    }
   ],
   "source": [
    "losses, ws = gradient_descent_mse(y_train, x_train, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Should be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#besoin pour SGD, prise de lab02\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celle du lab03, juste sans les degrés\n",
    "def plot_train_test(train_errors, test_errors, lambdas):\n",
    "    plt.semilogx(lambdas, train_errors, color='b', marker='*', label=\"Train error\")\n",
    "    plt.semilogx(lambdas, test_errors, color='r', marker='*', label=\"Test error\")\n",
    "    plt.xlabel(\"lambda\")\n",
    "    #plt.ylabel(\"RMSE\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Ridge regression for polynomial degree \")\n",
    "    leg = plt.legend(loc=1, shadow=True)\n",
    "    leg.draw_frame(False)\n",
    "    #plt.savefig(\"ridge_regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods to implement \n",
    "P.2 of project description\n",
    "\n",
    "-  least_square\n",
    "-  least_square_GD\n",
    "-  least_square_SGD\n",
    "\n",
    "-  ridge_regression\n",
    "-  logistic_regression\n",
    "-  reg_logistic_regression\n",
    "\n",
    "All function should return (w, loss) <br>\n",
    "Later add them in implementations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm. (probably change afterwards)\n",
    "max_iters = 200                                          \n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.random.rand(x_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least square "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_ls(y, tx, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "    \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    #print(e)\n",
    "    return (1/2)*np.mean(e**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = compute_loss_ls(y_train, x_train, w_initial)\n",
    "#print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"Compute the optimal w and the loss with least square technique\"\"\"\n",
    "    \"\"\"Required by project description\"\"\"\n",
    "    a = tx.T.dot(tx) \n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)                #p.7 du cours least squares\n",
    "    loss = compute_loss_ls(y, tx, w)                 #p.3 du cours least squares\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w is [-3.14664000e-01  2.93788272e-02 -2.52531475e-01 -2.54791124e-01\n",
      " -3.03696824e-02 -1.40144726e+00  2.95701641e-01 -1.07889472e+01\n",
      "  2.67880862e-01 -2.44934971e-03 -3.28827529e+02 -1.82647888e-01\n",
      "  1.14039627e-01  2.05045961e+01  6.38853340e+01 -3.18961885e-04\n",
      " -1.80884297e-03  6.29944860e+01 -4.48641524e-04  1.54379296e-03\n",
      "  1.21462701e-01  3.95268757e-04 -6.33223475e-02 -2.06747093e-01\n",
      " -1.16655767e-01  9.86256328e-02  1.67907718e-01 -3.35146253e-02\n",
      " -2.98358685e+00 -5.36388099e+00  2.78478918e+02] \n",
      " of shape (31,)\n",
      "\n",
      " loss of least squares is 0.33944559848933104\n"
     ]
    }
   ],
   "source": [
    "w_ls, loss_ls = least_squares(y_train, x_train)\n",
    "print(\"w is {} \\n of shape {}\".format(w_ls, w_ls.shape))\n",
    "print(\"\\n loss of least squares is {}\".format(loss_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_least_squares(y, tx, w):\n",
    "    \"\"\"Compute the gradient of least square.\"\"\"\n",
    "    '''\n",
    "    print(\"y is {}\".format(y.shape))\n",
    "    print(\"tx is {}\".format(tx.shape))\n",
    "    print(\"w is {}\".format(w.shape))\n",
    "    a = tx.dot(w)\n",
    "    print(\"a is {}\".format(a.shape))\n",
    "    '''\n",
    "    err = y - w.dot(tx.T) #tx.dot(w)\n",
    "    #print(err)\n",
    "    #print(err.shape)\n",
    "    grad = -tx.T.dot(err)/len(err)         #p.5 du cours least squares\n",
    "    return grad                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad is [ 0.82925627  1.35486047 -0.53388776  0.94381044  4.14668106  4.38643816\n",
      "  4.18672509  4.38796294 -1.70062676  2.54162102  4.84979533  0.4515363\n",
      "  3.09465883  4.38704467  2.11892879  1.32081617  0.78263535  1.92736598\n",
      "  1.28733905  0.52094746  2.58049624  0.74010854  4.69893771  5.15819561\n",
      "  4.67225708  4.51522475  4.51327119  4.41997267  4.38746095  4.38681567\n",
      "  4.8067236 ] \n",
      " of shape (31,)\n"
     ]
    }
   ],
   "source": [
    "#compute_gradient_least_square(y_train, x_train, w)\n",
    "grad_ls = compute_gradient_least_squares(y_train, x_train, w_initial)\n",
    "print(\"grad is {} \\n of shape {}\".format(grad_ls, grad_ls.shape))\n",
    "#print(\"\\nerr of least squares is {}\".format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.1\n",
    "def least_squares_GD(y, tx, w_initial, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm with least squares.\"\"\"\n",
    "    \"\"\"Required by project description\"\"\"\n",
    "    w = w_initial\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and error\n",
    "        grad = compute_gradient_least_squares(y, tx, w)\n",
    "        #print(grad)\n",
    "        # gradient w by descent update\n",
    "        w = w - (gamma * grad)\n",
    "        # print(w)\n",
    "        # calculate loss    \n",
    "    loss = compute_loss_ls(y, tx, w)           \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w is [-0.314664    0.03551621 -0.2632791  -0.23753145 -0.04211536 -0.08903241\n",
      "  0.38741625 -0.16497864  0.24720383 -0.02052449 -0.22774113 -0.12825728\n",
      "  0.1206671  -0.02182477  0.26182533 -0.00059858 -0.00106568  0.28433094\n",
      " -0.00091532  0.0027397   0.13953565  0.00115236 -0.0302467  -0.04043206\n",
      " -0.02335896 -0.09567177  0.20069583  0.19775776 -0.13618977 -0.00642423\n",
      "  0.02321595] \n",
      " of shape (31,)\n",
      "\n",
      " loss of least squares is 0.3410296670896865\n"
     ]
    }
   ],
   "source": [
    "w_ls_gd, loss_ls_gd = least_squares_GD(y_train, x_train, w_initial, max_iters, gamma)\n",
    "print(\"w is {} \\n of shape {}\".format(w_ls_gd, w_ls_gd.shape))\n",
    "print(\"\\n loss of least squares is {}\".format(loss_ls_gd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.01\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Stochastic Gradient Descent algorithm with least squares.\"\"\"\n",
    "    \"\"\"Required by project description\"\"\"\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad = compute_gradient_least_squares(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "    loss = compute_loss_ls(y, tx, w)                                 #TO CHECK p.3 least squares\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w is [-0.37166463 -0.00632278  0.1514792  -0.14046812 -0.34975606 -0.07375509\n",
      "  0.17958506 -0.14408062  0.23625826  0.11247356 -0.19644417 -0.06826166\n",
      "  0.31921351 -0.00507783  0.38774907  0.02588742  0.07996289  0.09645135\n",
      "  0.09720648 -0.06592488  0.142341    0.03936843  0.4962332   0.14426907\n",
      " -0.08692388 -0.15036215  0.14488584  0.21884451 -0.11744068  0.01120955\n",
      "  0.07366224] \n",
      " of shape (31,)\n",
      "\n",
      " loss of least squares is 0.567972586721236\n"
     ]
    }
   ],
   "source": [
    "w_ls_sgd, loss_ls_sgd = least_squares_SGD(y_train, x_train, w_initial, max_iters, gamma)\n",
    "print(\"w is {} \\n of shape {}\".format(w_ls_sgd, w_ls_sgd.shape))\n",
    "print(\"\\n loss of least squares is {}\".format(loss_ls_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should do grid search  TO DO\n",
    "lambda_ = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Je suis un peu confuse. Est ce que ce qu'il veut c'est ça ou bien ce que j'ai fait aux fonctions ridge_GD et ridge_SGD ?? \n",
    "# ridge_GD et ridge_SGD sont juste en-dessous\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"Ridge regression.\"\"\"\n",
    "    \"\"\"Required by project description\"\"\"\n",
    "    lambd = 2 * tx.shape[0] * lambda_\n",
    "    a = tx.T.dot(tx) + lambd * np.identity(tx.shape[1])\n",
    "    b = tx.T.dot(y) \n",
    "    w = np.linalg.solve(a, b) \n",
    "    err = y - tx.dot(w)\n",
    "    loss = (1/2) * np.mean(err**2) + lambda_ * (np.linalg.norm(w,2))**2   #TO CHECK p.3 ridge regression\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w is [-0.30849412  0.03572402 -0.24077855 -0.21610016 -0.01054923 -0.01919922\n",
      "  0.34814992 -0.03018983  0.23504977 -0.01013674 -0.00087247 -0.15674991\n",
      "  0.11455458 -0.02161131  0.18503225 -0.00076269 -0.00130455  0.23850622\n",
      " -0.00086979  0.00249252  0.10383837  0.00113401 -0.06198121 -0.14991608\n",
      "  0.03645409  0.04367213  0.0436772  -0.02328488 -0.02309077 -0.0234825\n",
      " -0.09703028] \n",
      " of shape (31,)\n",
      "\n",
      " loss of ridge regression is 0.3463797801150265\n"
     ]
    }
   ],
   "source": [
    "w_rreg, loss_rreg = ridge_regression(y_train, x_train, lambda_)\n",
    "print(\"w is {} \\n of shape {}\".format(w_rreg, w_rreg.shape))\n",
    "print(\"\\n loss of ridge regression is {}\".format(loss_rreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_ridge(y, tx, w, lambda_):\n",
    "    \"\"\"Not required specifically\"\"\" \n",
    "    err = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(err) / len(err) + (2 * lambda_* w)\n",
    "    return grad, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.1\n",
    "def ridge_GD(y, tx, initial_w, max_iters, gamma, lambda_):\n",
    "    \"\"\"Gradient descent algorithm with Ridge.\"\"\"\n",
    "    \"\"\"Not required specifically\"\"\" \n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and error\n",
    "        grad, _ = compute_gradient_ridge(y, tx, w, lambda_)\n",
    "        #print(\"grad is {} \\n of shape {}\".format(grad, grad.shape))\n",
    "        #print(\"\\n err of least squares is {}\".format(err))\n",
    "        # gradient w by descent update\n",
    "        w = w - gamma * grad\n",
    "        # calculate loss\n",
    "    loss = 1/2*np.mean((y - tx.dot(w))**2)            \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w is [-0.30849412  0.03969903 -0.24803349 -0.20668975 -0.02071613 -0.06276211\n",
      "  0.33439195 -0.11527076  0.22283479 -0.0216681  -0.15269138 -0.1237026\n",
      "  0.11820039 -0.01815791  0.23046428 -0.00072156 -0.00128755  0.24036147\n",
      " -0.00088598  0.00262998  0.11749501  0.00121341 -0.0389671  -0.05794899\n",
      " -0.00814204 -0.05584488  0.1427407   0.12822726 -0.09492894 -0.00800047\n",
      " -0.00835744] \n",
      " of shape (31,)\n",
      "\n",
      " loss of ridge_GD is 0.3416704527539797\n"
     ]
    }
   ],
   "source": [
    "w, loss = ridge_GD(y_train, x_train, w_initial, max_iters, gamma, lambda_)\n",
    "print(\"w is {} \\n of shape {}\".format(w, w.shape))\n",
    "print(\"\\n loss of ridge_GD is {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.01\n",
    "def ridge_SGD(y, tx, initial_w, max_iters, gamma, lambda_):\n",
    "    \"\"\"Stochastic Gradient Descent algorithm with least squares.\"\"\"\n",
    "    \"\"\"Not required specifically\"\"\" #Implicitely requested I think\n",
    "    \n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad, _ = compute_gradient_ridge(y_batch, tx_batch, w, lambda_)    \n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "    loss = np.mean((y - tx.dot(w))**2)/2 + lambda_ * (np.linalg.norm(w,2))**2\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w is [-0.16088127 -0.08597774  0.06764747 -0.72758024 -0.21006235 -0.01543221\n",
      "  0.2134124  -0.08507768  0.32333488  0.2422295  -0.08728427  0.147203\n",
      "  0.35983842  0.0499339   0.17484606 -0.03415117  0.18109739  0.18260342\n",
      "  0.04354737  0.09384829  0.11395921  0.01771609  0.61530534  0.21592138\n",
      " -0.04652917 -0.09264811  0.1921234   0.27806457 -0.05857891  0.06558203\n",
      "  0.21550393] \n",
      " of shape (31,)\n",
      "\n",
      " loss of ridge SGD is 1.472402904853191\n"
     ]
    }
   ],
   "source": [
    "w, loss = ridge_SGD(y_train, x_train, w_initial, max_iters, gamma, lambda_)\n",
    "print(\"w is {} \\n of shape {}\".format(w, w.shape))\n",
    "print(\"\\n loss of ridge SGD is {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-115-33f219d88e20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mrmse_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mridge_SGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_initial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mrmse_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloss_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mloss_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-113-0d6ead67e9ce>\u001b[0m in \u001b[0;36mridge_SGD\u001b[1;34m(y, tx, initial_w, max_iters, gamma, lambda_)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[1;31m# compute a stochastic gradient and loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_gradient_ridge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-f4e1e14b6520>\u001b[0m in \u001b[0;36mbatch_iter\u001b[1;34m(y, tx, batch_size, num_batches, shuffle)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mshuffle_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mshuffled_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mshuffled_tx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mshuffled_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lambdas = np.logspace(-4, -0.05, 15)\n",
    "\n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    w, loss_tr = ridge_SGD(y_train, x_train, w_initial, max_iters, gamma, lambda_)\n",
    "    rmse_tr.append(np.sqrt(2*loss_tr))\n",
    "    loss_te = (1/2) * np.mean((y_test - x_test.dot(w))**2) + lambda_ * (np.linalg.norm(w,2))**2\n",
    "    rmse_te.append(np.sqrt(2*loss_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search to choose lambda\n",
    "plot_train_test(rmse_tr, rmse_te, lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "Debugged but not sure from here: Il y aura probablement une correction de labo pour améliorer / re-vérifier les fonctions de logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Il y a un NaN dans la loss à cause d'un outlier. Je sais pas si on choisis d'enlever ou comment traiter. On verra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99981039 0.90820056 0.92786083 ... 0.28142199 0.00709906 0.04907172]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(tx, w):\n",
    "    \"\"\"Compute sigmoid function\"\"\"\n",
    "    z = np.array(np.exp(-tx.dot(w)))\n",
    "    return 1./(1 + z)\n",
    "\n",
    "value = sigmoid(x_train, w_initial)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logreg_loss(y, tx, w):  #np.log parce que math.log fonctionne pas.. J'ai toujours pas compris pourquoi.\n",
    "    \"\"\"Compute error and gradient of logistic regression\"\"\"\n",
    "    sig = sigmoid(tx, w)\n",
    "    loss = np.sum((-y * np.log(sig) - (1-y) * np.log(1-sig)), axis = -1)/len(y)\n",
    "    print(max(sig))\n",
    "    #a = (-y * np.log(sig) - (1-y) * np.log(1-sig))\n",
    "    #print(max(a))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.51459227 0.10073464 0.53488351 0.42669814 0.02003378 0.140064\n",
      " 0.2264286  0.072293   0.49416143 0.6004996  0.14155126 0.00350222\n",
      " 0.81835964 0.2093738  0.7436304  0.88667924 0.88977018 0.11193385\n",
      " 0.70287969 0.70028424 0.34716633 0.66417114 0.94782936 0.73171242\n",
      " 0.5700489  0.53273724 0.82729083 0.43374625 0.09599914 0.22535177\n",
      " 0.38780099]\n",
      "1.0\n",
      "loss is nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pauli\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Pauli\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in multiply\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "loss = compute_logreg_loss(y_train, x_train, w_initial)\n",
    "print(\"loss is {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logreg_grad(y, tx, w):\n",
    "    \"\"\"Compute error and gradient of logistic regression\"\"\" \n",
    "    sig = sigmoid(tx, w)\n",
    "    err  = sig - y\n",
    "    grad = tx.T.dot(err)/len(y)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_lr = compute_logreg_grad(y_train, x_train, w_initial)\n",
    "print(\"grad is {} \\n of shape {}\".format(grad_lr, grad_lr.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.01\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma): #SGD  (GD easy to implement from here)\n",
    "    \"\"\"Stochastic Gradient Descent algorithm with logistic regression.\"\"\"\n",
    "    \"\"\"Required by project description\"\"\"\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient\n",
    "            grad = compute_logreg_grad(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # compute a stochastic loss\n",
    "            compute_logreg_loss(y_batch, tx_batch, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_lr, loss_lr = logistic_regression(y_train, x_train, w_initial, max_iters, gamma)\n",
    "print(\"w is {} \\n of shape {}\".format(w_lr, w_lr.shape))\n",
    "print(\"\\n loss of ridge_SGD is {}\".format(loss_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logreg_reg_loss(y, tx, w, lambda_):\n",
    "    \"\"\"Compute error and gradient of logistic regression\"\"\"\n",
    "    reg = ( lambda_/(2*len(y)) ) * sum(w**2)\n",
    "    loss = compute_logreg_loss(y, tx, w) + reg\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_lrr = compute_logreg_reg_loss(y_train, x_train, w_initial, lambda_)\n",
    "print(\"loss is {}\".format(loss_lrr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logreg_reg_grad(y, tx, w, lambda_):\n",
    "    \"\"\"Compute error and gradient of logistic regression\"\"\"     \n",
    "    grad = compute_logreg_grad(y, tx, w) \n",
    "    reg = (lambda_/len(y)) * w[1:]\n",
    "    grad[1:] = grad[1:] + reg            \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_lrr = compute_logreg_reg_grad(y_train, x_train, w_initial, lambda_)\n",
    "print(\"grad is {} \\n of shape {}\".format(grad_lrr, grad_lrr.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# La loss de ridge SGD me semble particulièrement haute. A vérifier avec les plots (et/ou code).\n",
    "gamma = 0.01\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    \"\"\"Stochastic Gradient Descent algorithm with REGULARIZED logistic regression.\"\"\"\n",
    "    \"\"\"Required by project description\"\"\"\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient\n",
    "            grad = compute_logreg_reg_grad(y_batch, tx_batch, w, lambda_)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # compute a stochastic loss\n",
    "            loss = compute_logreg_reg_loss(y_batch, tx_batch, w, lambda_)\n",
    "    return w_rlr_sgd, loss_rlr_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_rlr_sgd, loss_rlr_sgd = reg_logistic_regression(y_train, x_train, lambda_, w_initial, max_iters, gamma)\n",
    "print(\"w is {} \\n of shape {}\".format(w_rlr_sgd, w_rlr_sgd.shape))\n",
    "print(\"\\n loss of ridge_SGD is {}\".format(loss_rlr_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction de chaque méthode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = predict_labels(weights, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Grid search pour meilleure gamma et lambda pour chaque méthode (pour lambda faire en meme temps que lambda (influence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choisir méthode:\n",
    "(D'après moi mais hésitez pas à faire autrement si vous avez une meilleure idée)\n",
    "- Pour chaque méthode: faire un plot de test_error (=loss) pour un grand nombre d'iteration (assez pour que on voit que l'erreur commence à remonter si possible).  Il faudrait idealement voir la courbe descendre et monter dans le graphe (underfit à overfit)\n",
    "- Garder la valeur de n iter ou la test error était minimale (au cas où: risque d'être très long pour ridge si lambda_ trop grand)\n",
    "- Comparer les loss de chaque méthode à leur n_iter optimale\n",
    "- La meilleure est celle ou c'est le plus faible\n",
    "\n",
    "En plus comme ça on aura des beau plots et des arguments pour le rapport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
