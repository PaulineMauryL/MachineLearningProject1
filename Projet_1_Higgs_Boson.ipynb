{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import load_csv_data, predict_labels, create_csv_submission\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "(250000, 30)\n"
     ]
    }
   ],
   "source": [
    "y_train, input_train, ids_train = load_csv_data('train.csv', sub_sample=False)\n",
    "print(y_train.shape)\n",
    "print(input_train.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_test, input_test, ids_test = load_csv_data('test.csv', sub_sample=False)\n",
    "print(y_test.shape)\n",
    "print(input_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_train(x):\n",
    "    ''' standardize training set\n",
    "    '''\n",
    "    centered_data = x - np.mean(x, axis=0)\n",
    "    std_data = centered_data / np.std(x, axis=0)\n",
    "    \n",
    "    return std_data, np.mean(x, axis=0), np.std(x, axis=0)\n",
    "\n",
    "def standardize_test(x, mean, std):\n",
    "    ''' standardize test set with same values as training set\n",
    "    '''\n",
    "    return (x-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEE: on pourrait ausi tenter build_polynomial comme dans les déries si on est motivés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise data\n",
    "# Careful to standardize the x_test with the mean and std of x_train\n",
    "x_train, mean, std = standardize_train(input_train)\n",
    "#x_test = standardize_test(input_test, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "#print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 138.47 ,   51.655,   97.827, ...,    1.24 ,   -2.475,  113.497],\n",
       "       [ 160.937,   68.768,  103.235, ..., -999.   , -999.   ,   46.226],\n",
       "       [-999.   ,  162.172,  125.953, ..., -999.   , -999.   ,   44.251],\n",
       "       ...,\n",
       "       [ 105.457,   60.526,   75.839, ..., -999.   , -999.   ,   41.992],\n",
       "       [  94.951,   19.362,   68.812, ..., -999.   , -999.   ,    0.   ],\n",
       "       [-999.   ,   72.756,   70.831, ..., -999.   , -999.   ,    0.   ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE method GD and SGD from lab02\n",
    "I put it here to use as reference for the function to implement.\n",
    "\n",
    "Could try it too  --> will be less good than Ridge if find good lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm. (probably change afterwards)\n",
    "max_iters = 5                                                    ############ MEGA LONG (baisser si jamais)\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.ones(x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From lab02 correction\n",
    "# MSE GD and SGD\n",
    "def calculate_mse(e):\n",
    "    \"\"\"Calculate the mse for vector e.\"\"\"\n",
    "    return np.mean(e**2)/2\n",
    "\n",
    "def compute_loss_mse(y, tx, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "\n",
    "    You can calculate the loss using mse or mae.\n",
    "    \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    return calculate_mse(e)\n",
    "\n",
    "def compute_gradient_mse(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    err = y - tx.dot(w)\n",
    "    #print(err)\n",
    "    #print(len(err))\n",
    "    #print(-tx.T.dot(err))\n",
    "    grad = -tx.T.dot(err) / len(err)\n",
    "    return grad, err\n",
    "\n",
    "def gradient_descent_mse(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute loss, gradient\n",
    "        grad, err = compute_gradient_mse(y, tx, w)\n",
    "        loss = calculate_mse(err)\n",
    "        # gradient w by descent update\n",
    "        w = w - gamma * grad\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws\n",
    "\n",
    "def stochastic_gradient_descent(\n",
    "        y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad, _ = compute_gradient_mse(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "            loss = compute_loss_mse(y, tx, w)\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        print(\"SGD({bi}/{ti}): loss={l}\".format(bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/9): loss=0.5, w0=0.15890072268734834, w1=-0.23350355904089673\n",
      "Gradient Descent(1/9): loss=1.3151906775211355, w0=-0.12798293911466097, w1=0.05539970420684712\n",
      "Gradient Descent(2/9): loss=51.21992783427874, w0=1.5545037905755277, w1=-1.6951184187302437\n",
      "Gradient Descent(3/9): loss=2924.809070536689, w0=-11.051329809331301, w1=11.09869204845918\n",
      "Gradient Descent(4/9): loss=168340.19612585776, w0=84.39306689457838, w1=-85.82113308277333\n",
      "Gradient Descent(5/9): loss=9690293.438496064, w0=-639.7203788705657, w1=649.3098422008295\n",
      "Gradient Descent(6/9): loss=557810954.067767, w0=4854.103153077977, w1=-4928.157753964412\n",
      "Gradient Descent(7/9): loss=32109767876.289787, w0=-36828.015945113555, w1=37388.47513051421\n",
      "Gradient Descent(8/9): loss=1848363117047.5952, w0=279417.80295701657, w1=-283671.50264545856\n",
      "Gradient Descent(9/9): loss=106398969486965.27, w0=-2119966.7483497383, w1=2152238.427913745\n"
     ]
    }
   ],
   "source": [
    "losses, ws = gradient_descent_mse(y_train, x_train, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Should be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#besoin pour SGD, prise de lab02\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celle du lab03, juste sans les degrés\n",
    "def plot_train_test(train_errors, test_errors, lambdas):\n",
    "    plt.semilogx(lambdas, train_errors, color='b', marker='*', label=\"Train error\")\n",
    "    plt.semilogx(lambdas, test_errors, color='r', marker='*', label=\"Test error\")\n",
    "    plt.xlabel(\"lambda\")\n",
    "    #plt.ylabel(\"RMSE\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Ridge regression for polynomial degree \")\n",
    "    leg = plt.legend(loc=1, shadow=True)\n",
    "    leg.draw_frame(False)\n",
    "    #plt.savefig(\"ridge_regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods to implement \n",
    "P.2 of project description\n",
    "\n",
    "-  least_square\n",
    "-  least_square_GD\n",
    "-  least_square_SGD\n",
    "\n",
    "-  ridge_regression\n",
    "-  logistic_regression\n",
    "-  reg_logistic_regression\n",
    "\n",
    "All function should return (w, loss) <br>\n",
    "Later add them in implementations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm. (probably change afterwards)\n",
    "max_iters = 10                                          \n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(x_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least square "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_ls(y, tx, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "    \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    return 1/2*np.mean(e**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"Compute the optimal w and the loss with least square technique\"\"\"\n",
    "    \"\"\"Required by project description\"\"\"\n",
    "    a = tx.T.dot(tx) \n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)                #p.7 du cours least squares\n",
    "    loss = compute_loss_ls(y, tx, w)                 #p.3 du cours least squares\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w is [ 2.93788275e-02 -2.52531475e-01 -2.54791125e-01 -3.03696825e-02\n",
      " -1.40144671e+00  2.95701639e-01 -1.07889471e+01  2.67880862e-01\n",
      " -2.44934960e-03 -3.28843215e+02 -1.82647888e-01  1.14039627e-01\n",
      "  2.05045956e+01  6.38883723e+01 -3.18961843e-04 -1.80884298e-03\n",
      "  6.29974773e+01 -4.48641621e-04  1.54379308e-03  1.21462700e-01\n",
      "  3.95268639e-04 -6.33223479e-02 -2.06747092e-01 -1.16655759e-01\n",
      "  9.86256238e-02  1.67907719e-01 -3.35146169e-02 -2.98358686e+00\n",
      " -5.36388110e+00  2.78492205e+02] \n",
      " of shape (30,)\n",
      "\n",
      " loss of least squares is 0.3889523149372305\n"
     ]
    }
   ],
   "source": [
    "w_ls, loss_ls = least_squares(y_train, x_train)\n",
    "print(\"w is {} \\n of shape {}\".format(w_ls, w_ls.shape))\n",
    "print(\"\\n loss of least squares is {}\".format(loss_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_least_squares(y, tx, w):\n",
    "    \"\"\"Compute the gradient of least square.\"\"\"\n",
    "    '''\n",
    "    print(\"y is {}\".format(y.shape))\n",
    "    print(\"tx is {}\".format(tx.shape))\n",
    "    print(\"w is {}\".format(w.shape))\n",
    "    a = tx.dot(w)\n",
    "    print(\"a is {}\".format(a.shape))\n",
    "    '''\n",
    "    err = y - w.dot(tx.T)#tx.dot(w)\n",
    "    #print(err)\n",
    "    #print(err.shape)\n",
    "    grad = -(tx.T @ err) / len(err)          #p.5 du cours least squares\n",
    "    return grad                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad is [-0.22700103  0.33357651  0.01334131 -0.18274659 -0.13445082 -0.18202497\n",
      " -0.13341468 -0.01162345  0.01451087 -0.14545203  0.18547229 -0.25794773\n",
      " -0.13416605 -0.22328862  0.00089534  0.0041789   0.03032475 -0.00143922\n",
      " -0.00391589 -0.02132456 -0.00709562 -0.12863626 -0.12676524 -0.14959835\n",
      " -0.14282589 -0.14282473 -0.13356652 -0.1340891  -0.13408034 -0.12747392] \n",
      " of shape (30,)\n"
     ]
    }
   ],
   "source": [
    "#compute_gradient_least_square(y_train, x_train, w)\n",
    "grad_ls = compute_gradient_least_squares(y_train, x_train, w_initial)\n",
    "print(\"grad is {} \\n of shape {}\".format(grad_ls, grad_ls.shape))\n",
    "#print(\"\\nerr of least squares is {}\".format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, w_initial, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm with least squares.\"\"\"\n",
    "    \"\"\"Required by project description\"\"\"\n",
    "    w = w_initial\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and error\n",
    "        grad = compute_gradient_least_squares(y, tx, w)\n",
    "        print(grad)\n",
    "        # gradient w by descent update\n",
    "        w = w - (gamma * grad)\n",
    "        # print(w)\n",
    "        # calculate loss    \n",
    "    loss = compute_loss_ls(y, tx, w)           \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.22700103  0.33357651  0.01334131 -0.18274659 -0.13445082 -0.18202497\n",
      " -0.13341468 -0.01162345  0.01451087 -0.14545203  0.18547229 -0.25794773\n",
      " -0.13416605 -0.22328862  0.00089534  0.0041789   0.03032475 -0.00143922\n",
      " -0.00391589 -0.02132456 -0.00709562 -0.12863626 -0.12676524 -0.14959835\n",
      " -0.14282589 -0.14282473 -0.13356652 -0.1340891  -0.13408034 -0.12747392]\n",
      "[ 4.09833803e-01 -4.12718947e-01 -3.19436834e-02  1.00423868e+00\n",
      "  1.19549526e+00  1.12224957e+00  1.19614187e+00 -5.84514813e-01\n",
      "  4.80440258e-01  1.14057349e+00 -4.51196927e-02  7.56294840e-01\n",
      "  1.19570789e+00  4.46125732e-01  1.01180311e-02  5.04438891e-03\n",
      "  2.20402526e-01  2.18216906e-02 -6.63131742e-04  5.14281933e-01\n",
      "  1.41272120e-02  1.09991388e+00  1.28921671e+00  1.07225450e+00\n",
      "  1.02897974e+00  1.02898049e+00  1.20256092e+00  1.19575741e+00\n",
      "  1.19575608e+00  1.19480454e+00]\n",
      "[-2.40355247  2.50074018  0.39329383 -7.71054727 -9.19524218 -8.95951789\n",
      " -9.19385953  4.32670567 -3.67810119 -8.7040835  -0.49328688 -5.44884171\n",
      " -9.19514658 -3.20201509 -0.07133065 -0.04742778 -2.69442824 -0.15146065\n",
      "  0.01136998 -4.55699686 -0.08194829 -8.17675039 -9.53979149 -7.81023278\n",
      " -7.46386138 -7.4639025  -9.24176897 -9.19504879 -9.19499343 -8.93632214]\n",
      "[ 18.00833371 -18.2768721   -2.6218817   58.47937359  69.85895867\n",
      "  67.75587671  69.854922   -33.08523124  28.01047867  66.17502835\n",
      "   4.22850103  40.94141236  69.86002634  23.9846639    0.52780595\n",
      "   0.36164356  20.59928274   1.13055339  -0.0917319   34.61960866\n",
      "   0.61465414  62.24100091  72.46115262  59.02490707  56.37270093\n",
      "  56.37300738  70.22254491  69.85977948  69.85941532  67.99717111]\n",
      "[-136.34913815  138.45689304   19.99349558 -443.665109   -530.06502606\n",
      " -514.38726421 -530.02885364  250.81092352 -212.62322838 -502.09382967\n",
      "  -32.34006998 -310.47494325 -530.071626   -182.02036567   -3.99681905\n",
      "   -2.74797584 -156.80549902   -8.56730121    0.69925994 -262.9326797\n",
      "   -4.65339917 -472.13145385 -549.63476268 -447.72680433 -427.60430103\n",
      " -427.60665957 -532.8191782  -530.06931493 -530.06650308 -515.79482118]\n",
      "[ 1034.44777966 -1050.18710755  -151.42107094  3366.15205614\n",
      "  4021.69985022  3902.4821636   4021.43086039 -1903.10623865\n",
      "  1613.17546551  3809.48866334   245.58834214  2355.46719095\n",
      "  4021.75140776  1380.83298644    30.31901115    20.84850184\n",
      "  1189.65679472    64.9934567     -5.30713874  1994.86327993\n",
      "    35.3032835   3582.21468795  4170.20207228  3396.82616528\n",
      "  3244.14116338  3244.15904392  4042.60111724  4021.73431166\n",
      "  4021.71303298  3913.49816998]\n",
      "[ -7848.31933136   7967.81085166   1148.9826067  -25539.24376812\n",
      " -30512.99127384 -29608.72252075 -30510.94547297  14438.85638552\n",
      " -12239.37886633 -28902.92574783  -1863.3715467  -17871.06201141\n",
      " -30513.38112073 -10476.56722057   -230.0300801    -158.18152028\n",
      "  -9026.33276444   -493.1071075      40.26706612 -15135.33644443\n",
      "   -267.84502121 -27178.50945468 -31639.59087176 -25771.99686691\n",
      " -24613.56621564 -24613.70190013 -30671.56832316 -30513.25100263\n",
      " -30513.08950629 -29691.97199827]\n",
      "[  59545.88442599  -60452.33269211   -8717.22793633  193768.5045125\n",
      "  231504.75040582  224643.75028739  231489.23350008 -109549.00799201\n",
      "   92861.19832336  219289.03549117   14137.67719247  135589.26150854\n",
      "  231507.70947956   79486.51862732    1745.25613674    1200.13603116\n",
      "   68483.46374992    3741.24373796    -305.51034936  114833.06808199\n",
      "    2032.16280287  206205.78037647  240052.39986967  195534.31957729\n",
      "  186745.19735616  186746.22679173  232707.89095201  231506.72266307\n",
      "  231505.49743255  225275.65137857]\n",
      "[ -451779.74128876   458657.11110853    66138.50672941 -1470138.45325137\n",
      " -1756446.67333813 -1704391.88570619 -1756328.94079712   831157.71127847\n",
      "  -704546.04156527 -1663764.99089968  -107263.78610955 -1028727.48621583\n",
      " -1756469.12292029  -603071.19304785   -13241.4092422     -9105.53783994\n",
      "  -519590.20044728   -28385.14020914     2317.9341606   -871248.55933989\n",
      "   -15418.19414727 -1564501.15197737 -1821298.36512859 -1483535.90215039\n",
      " -1416852.07114172 -1416859.88157593 -1765575.00381499 -1756461.63547569\n",
      " -1756452.33948903 -1709185.90695635]\n",
      "[ 3427692.21615251 -3479871.32937029  -501798.44600122 11154068.38620446\n",
      " 13326313.73842928 12931369.33115254 13325420.49561757 -6306065.98775538\n",
      "  5345452.0790309  12623129.73674805   813819.74959     7805044.925184\n",
      " 13326484.06646845  4575553.53487184   100463.72287765    69084.50685537\n",
      "  3942175.93286945   215360.52715915   -17586.36832608  6610238.56967474\n",
      "   116979.18030566 11870006.38566276 13818349.20915951 11255715.91372352\n",
      " 10749779.88138923 10749839.13982054 13395571.17722234 13326427.25894739\n",
      " 13326356.72954587 12967742.22351458]\n",
      "[-2.60061990e+07  2.64020865e+07  3.80718859e+06 -8.46268872e+07\n",
      " -1.01107902e+08 -9.81114243e+07 -1.01101125e+08  4.78446711e+07\n",
      " -4.05564100e+07 -9.57727835e+07 -6.17452121e+06 -5.92175548e+07\n",
      " -1.01109194e+08 -3.47151228e+07 -7.62227005e+05 -5.24150165e+05\n",
      " -2.99096319e+07 -1.63395905e+06  1.33429307e+05 -5.01524552e+07\n",
      " -8.87531217e+05 -9.00587711e+07 -1.04841018e+08 -8.53980959e+07\n",
      " -8.15595152e+07 -8.15599648e+07 -1.01633364e+08 -1.01108763e+08\n",
      " -1.01108228e+08 -9.83873884e+07]\n",
      "[ 1.97311294e+08 -2.00314927e+08 -2.88854709e+07  6.42071555e+08\n",
      "  7.67114446e+08  7.44379911e+08  7.67063028e+08 -3.63001682e+08\n",
      "  3.07705011e+08  7.26636440e+08  4.68466297e+07  4.49288740e+08\n",
      "  7.67124251e+08  2.63386657e+08  5.78308260e+06  3.97677290e+06\n",
      "  2.26926979e+08  1.23969895e+07 -1.01233976e+06  3.80511040e+08\n",
      "  6.73377656e+06  6.83283731e+08  7.95437921e+08  6.47922783e+08\n",
      "  6.18799137e+08  6.18802548e+08  7.71101174e+08  7.67120981e+08\n",
      "  7.67116921e+08  7.46473675e+08]\n",
      "[-1.49701796e+09  1.51980678e+09  2.19156581e+08 -4.87145275e+09\n",
      " -5.82016404e+09 -5.64767515e+09 -5.81977392e+09  2.75412534e+09\n",
      " -2.33458469e+09 -5.51305389e+09 -3.55429455e+08 -3.40879276e+09\n",
      " -5.82023843e+09 -1.99833748e+09 -4.38767508e+07 -3.01721219e+07\n",
      " -1.72171473e+09 -9.40570378e+07  7.68070981e+06 -2.88697037e+09\n",
      " -5.10897485e+07 -5.18413311e+09 -6.03505671e+09 -4.91584652e+09\n",
      " -4.69488289e+09 -4.69490877e+09 -5.85041169e+09 -5.82021362e+09\n",
      " -5.82018282e+09 -5.66356071e+09]\n",
      "[ 1.13580055e+10 -1.15309063e+10 -1.66276005e+09  3.69601358e+10\n",
      "  4.41580909e+10  4.28494027e+10  4.41551310e+10 -2.08957886e+10\n",
      "  1.77126971e+10  4.18280195e+10  2.69667419e+09  2.58628072e+10\n",
      "  4.41586553e+10  1.51615603e+10  3.32896725e+08  2.28918514e+08\n",
      "  1.30627995e+10  7.13618929e+08 -5.82742136e+07  2.19036953e+10\n",
      "  3.87622367e+08  3.93324689e+10  4.57885003e+10  3.72969552e+10\n",
      "  3.56204849e+10  3.56206813e+10  4.43875824e+10  4.41584671e+10\n",
      "  4.41582334e+10  4.29699278e+10]\n",
      "[-8.61741756e+10  8.74859890e+10  1.26155051e+10 -2.80419766e+11\n",
      " -3.35031277e+11 -3.25102146e+11 -3.35008821e+11  1.58538166e+11\n",
      " -1.34387774e+11 -3.17352823e+11 -2.04599017e+10 -1.96223368e+11\n",
      " -3.35035559e+11 -1.15032077e+11 -2.52571642e+09 -1.73682468e+09\n",
      " -9.91085960e+10 -5.41428889e+09  4.42131528e+08 -1.66185242e+11\n",
      " -2.94092462e+09 -2.98418863e+11 -3.47401335e+11 -2.82975244e+11\n",
      " -2.70255718e+11 -2.70257208e+11 -3.36772450e+11 -3.35034131e+11\n",
      " -3.35032358e+11 -3.26016581e+11]\n",
      "[ 6.53810967e+11 -6.63763809e+11 -9.57149349e+10  2.12756916e+12\n",
      "  2.54191145e+12  2.46657826e+12  2.54174107e+12 -1.20284286e+12\n",
      "  1.01961173e+12  2.40778347e+12  1.55231054e+11  1.48876377e+12\n",
      "  2.54194394e+12  8.72758375e+11  1.91628302e+10  1.31774400e+10\n",
      "  7.51945540e+11  4.10786809e+10 -3.35449037e+09  1.26086189e+12\n",
      "  2.23130509e+10  2.26412988e+12  2.63576415e+12  2.14695779e+12\n",
      "  2.05045364e+12  2.05046495e+12  2.55512187e+12  2.54193310e+12\n",
      "  2.54191965e+12  2.47351616e+12]\n",
      "[-4.96052069e+12  5.03603377e+12  7.26197538e+11 -1.61420523e+13\n",
      " -1.92857033e+13 -1.87141439e+13 -1.92844106e+13  9.12607340e+12\n",
      " -7.73588292e+12 -1.82680627e+13 -1.17775151e+12 -1.12953802e+13\n",
      " -1.92859498e+13 -6.62169372e+12 -1.45390060e+11 -9.99783834e+10\n",
      " -5.70507623e+12 -3.11667525e+11  2.54508102e+10 -9.56626884e+12\n",
      " -1.69291059e+11 -1.71781504e+13 -1.99977719e+13 -1.62891556e+13\n",
      " -1.55569702e+13 -1.55570559e+13 -1.93859319e+13 -1.92858676e+13\n",
      " -1.92857655e+13 -1.87667823e+13]\n",
      "[ 3.76359020e+13 -3.82088263e+13 -5.50972390e+12  1.22471155e+14\n",
      "  1.46322309e+14  1.41985838e+14  1.46312501e+14 -6.92403128e+13\n",
      "  5.86928167e+13  1.38601381e+14  8.93570319e+12  8.56990324e+13\n",
      "  1.46324179e+14  5.02393663e+13  1.10308703e+12  7.58544692e+11\n",
      "  4.32849097e+13  2.36464863e+12 -1.93097511e+11  7.25801139e+13\n",
      "  1.28442599e+12  1.30332121e+14  1.51724835e+14  1.23587240e+14\n",
      "  1.18032086e+14  1.18032737e+14  1.47082752e+14  1.46323555e+14\n",
      "  1.46322781e+14  1.42385210e+14]\n",
      "[-2.85546863e+14  2.89893689e+14  4.18027545e+13 -9.29199308e+14\n",
      " -1.11016009e+15 -1.07725890e+15 -1.11008567e+15  5.25332277e+14\n",
      " -4.45307507e+14 -1.05158073e+15 -6.77959575e+13 -6.50206014e+14\n",
      " -1.11017427e+15 -3.81170443e+14 -8.36921728e+12 -5.75514457e+12\n",
      " -3.28406375e+14 -1.79407949e+13  1.46504762e+12 -5.50671639e+14\n",
      " -9.74505173e+12 -9.88841142e+14 -1.15114953e+15 -9.37667144e+14\n",
      " -8.95519706e+14 -8.95524642e+14 -1.11592964e+15 -1.11016954e+15\n",
      " -1.11016367e+15 -1.08028898e+15]\n",
      "[ 2.16646889e+15 -2.19944864e+15 -3.17161135e+14  7.04991599e+15\n",
      "  8.42288115e+15  8.17325701e+15  8.42231658e+15 -3.98574169e+15\n",
      "  3.37858679e+15  7.97843452e+15  5.14373828e+14  4.93316960e+15\n",
      "  8.42298880e+15  2.89197331e+15  6.34979796e+13  4.36647825e+13\n",
      "  2.49164774e+15  1.36118371e+14 -1.11154437e+13  4.17799363e+15\n",
      "  7.39365552e+13  7.50242377e+15  8.73387159e+15  7.11416220e+15\n",
      "  6.79438591e+15  6.79442337e+15  8.46665523e+15  8.42295290e+15\n",
      "  8.42290832e+15  8.19624644e+15]\n",
      "[-1.64371879e+16  1.66874082e+16  2.40632913e+15 -5.34883259e+16\n",
      " -6.39051320e+16 -6.20112120e+16 -6.39008486e+16  3.02401689e+16\n",
      " -2.56336319e+16 -6.05330768e+16 -3.90259898e+15 -3.74283869e+16\n",
      " -6.39059488e+16 -2.19416531e+16 -4.81764695e+14 -3.31288504e+14\n",
      " -1.89043482e+16 -1.03274193e+15  8.43338386e+13 -3.16988011e+16\n",
      " -5.60963077e+14 -5.69215419e+16 -6.62646436e+16 -5.39757675e+16\n",
      " -5.15495970e+16 -5.15498812e+16 -6.42372498e+16 -6.39056764e+16\n",
      " -6.39053382e+16 -6.21856348e+16]\n",
      "[ 1.24710375e+17 -1.26608818e+17 -1.82570284e+16  4.05820582e+17\n",
      "  4.84853796e+17  4.70484460e+17  4.84821297e+17 -2.29434793e+17\n",
      "  1.94484595e+17  4.59269720e+17  2.96093580e+16  2.83972428e+17\n",
      "  4.84859993e+17  1.66473231e+17  3.65519065e+15  2.51351470e+15\n",
      "  1.43428934e+17  7.83550284e+15 -6.39848170e+14  2.40501562e+17\n",
      "  4.25607567e+15  4.31868690e+17  5.02755616e+17  4.09518844e+17\n",
      "  3.91111277e+17  3.91113433e+17  4.87373603e+17  4.84857927e+17\n",
      "  4.84855361e+17  4.71807821e+17]\n",
      "[-9.46188463e+17  9.60592114e+17  1.38517663e+17 -3.07899606e+18\n",
      " -3.67862793e+18 -3.56960653e+18 -3.67838136e+18  1.74074173e+18\n",
      " -1.47557155e+18 -3.48451932e+18 -2.24648775e+17 -2.15452351e+18\n",
      " -3.67867495e+18 -1.26304689e+18 -2.77322495e+16 -1.90702547e+16\n",
      " -1.08820780e+18 -5.94486417e+16  4.85458373e+15 -1.82470627e+18\n",
      " -3.22912163e+16 -3.27662532e+18 -3.81445060e+18 -3.10705509e+18\n",
      " -2.96739529e+18 -2.96741164e+18 -3.69774593e+18 -3.67865927e+18\n",
      " -3.67863980e+18 -3.57964698e+18]\n",
      "[ 7.17881419e+18 -7.28809594e+18 -1.05094556e+18  2.33606110e+19\n",
      "  2.79100702e+19  2.70829153e+19  2.79081994e+19 -1.32071590e+19\n",
      "  1.11952897e+19  2.64373512e+19  1.70442980e+18  1.63465573e+19\n",
      "  2.79104269e+19  9.58284661e+18  2.10406989e+17  1.44687681e+17\n",
      "  8.25632726e+18  4.51042017e+17 -3.68321491e+16  1.38442052e+19\n",
      "  2.44996268e+17  2.48600414e+19  2.89405685e+19  2.35734973e+19\n",
      "  2.25138862e+19  2.25140103e+19  2.80551200e+19  2.79103079e+19\n",
      "  2.79101602e+19  2.71590931e+19]\n",
      "[-5.44662878e+19  5.52954179e+19  7.97361539e+18 -1.77238988e+20\n",
      " -2.11756130e+20 -2.05480434e+20 -2.11741937e+20  1.00203864e+20\n",
      " -8.49396368e+19 -2.00582484e+20 -1.29316572e+19 -1.24022753e+20\n",
      " -2.11758837e+20 -7.27058909e+19 -1.59637613e+18 -1.09775803e+18\n",
      " -6.26414732e+19 -3.42209502e+18  2.79448720e+17 -1.05037190e+20\n",
      " -1.85880800e+18 -1.88615297e+20 -2.19574611e+20 -1.78854175e+20\n",
      " -1.70814813e+20 -1.70815755e+20 -2.12856636e+20 -2.11757934e+20\n",
      " -2.11756814e+20 -2.06058402e+20]\n",
      "[ 4.13240464e+20 -4.19531146e+20 -6.04965136e+19  1.34472762e+21\n",
      "  1.60661218e+21  1.55899793e+21  1.60650449e+21 -7.60255433e+20\n",
      "  6.44444414e+20  1.52183675e+21  9.81136079e+19  9.40971413e+20\n",
      "  1.60663272e+21  5.51625919e+20  1.21118446e+19  8.32878565e+18\n",
      "  4.75266307e+20  2.59637326e+19 -2.12020175e+18  7.96926295e+20\n",
      "  1.41029380e+19  1.43104067e+21  1.66593168e+21  1.35698218e+21\n",
      "  1.29598685e+21  1.29599399e+21  1.61496182e+21  1.60662587e+21\n",
      "  1.60661737e+21  1.56338302e+21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.13529135e+21  3.18301931e+21  4.58992312e+20 -1.02025654e+22\n",
      " -1.21895064e+22 -1.18282529e+22 -1.21886894e+22  5.76812411e+21\n",
      " -4.88945583e+21 -1.15463078e+22 -7.44396479e+20 -7.13923197e+21\n",
      " -1.21896622e+22 -4.18523384e+21 -9.18936187e+19 -6.31912213e+19\n",
      " -3.60588682e+21 -1.96989098e+20  1.60861551e+19 -6.04634913e+21\n",
      " -1.07000217e+20 -1.08574300e+22 -1.26395686e+22 -1.02955419e+22\n",
      " -9.83276496e+21 -9.83281916e+21 -1.22528558e+22 -1.21896102e+22\n",
      " -1.21895457e+22 -1.18615230e+22]\n",
      "[ 2.37877282e+22 -2.41498444e+22 -3.48241460e+21  7.74077516e+22\n",
      "  9.24828456e+22  8.97419840e+22  9.24766466e+22 -4.37632595e+22\n",
      "  3.70967268e+22  8.76028421e+22  5.64780084e+21  5.41659741e+22\n",
      "  9.24840276e+22  3.17537332e+22  6.97204880e+20  4.79437293e+20\n",
      "  2.73581770e+22  1.49457343e+21 -1.22047058e+20  4.58741769e+22\n",
      "  8.11819958e+20  8.23762661e+22  9.58975063e+22  7.81131720e+22\n",
      "  7.46020432e+22  7.46024544e+22  9.29634830e+22  9.24836334e+22\n",
      "  9.24831439e+22  8.99944069e+22]\n",
      "[-1.80479564e+23  1.83226970e+23  2.64213825e+22 -5.87299346e+23\n",
      " -7.01675395e+23 -6.80880240e+23 -7.01628363e+23  3.32035657e+23\n",
      " -2.81456093e+23 -6.64650384e+23 -4.28503563e+22 -4.10961958e+23\n",
      " -7.01684363e+23 -2.40918337e+23 -5.28975408e+21 -3.63753246e+21\n",
      " -2.07568869e+23 -1.13394586e+22  9.25981649e+20 -3.48051371e+23\n",
      " -6.15934865e+21 -6.24995898e+23 -7.27582723e+23 -5.92651431e+23\n",
      " -5.66012192e+23 -5.66015312e+23 -7.05322033e+23 -7.01681372e+23\n",
      " -7.01677659e+23 -6.82795394e+23]\n",
      "[ 1.36931415e+24 -1.39015896e+24 -2.00461328e+23  4.45589123e+24\n",
      "  5.32367227e+24  5.16589762e+24  5.32331543e+24 -2.51918341e+24\n",
      "  2.13543187e+24  5.04276029e+24  3.25109381e+23  3.11800412e+24\n",
      "  5.32374031e+24  1.82786838e+24  4.01338244e+22  2.75982752e+22\n",
      "  1.57484307e+24  8.60334589e+22 -7.02550333e+21  2.64069603e+24\n",
      "  4.67315140e+22  4.74189825e+24  5.52023342e+24  4.49649797e+24\n",
      "  4.29438374e+24  4.29440741e+24  5.35133963e+24  5.32371762e+24\n",
      "  5.32368944e+24  5.18042806e+24]\n",
      "[-1.03891056e+25  1.05472570e+25  1.52091753e+24 -3.38072344e+25\n",
      " -4.03911646e+25 -3.91941146e+25 -4.03884573e+25  1.91132637e+25\n",
      " -1.62017073e+25 -3.82598610e+25 -2.46663316e+24 -2.36565685e+25\n",
      " -4.03916809e+25 -1.38681964e+25 -3.04498817e+23 -2.09390515e+23\n",
      " -1.19484713e+25 -6.52743337e+23  5.33031049e+22 -2.00351905e+25\n",
      " -3.54556062e+23 -3.59771945e+25 -4.18824912e+25 -3.41153213e+25\n",
      " -3.25818630e+25 -3.25820426e+25 -4.06010793e+25 -4.03915087e+25\n",
      " -4.03912949e+25 -3.93043584e+25]\n",
      "[ 7.88230479e+25 -8.00229564e+25 -1.15393337e+25  2.56498429e+26\n",
      "  3.06451280e+26  2.97369158e+26  3.06430739e+26 -1.45013995e+26\n",
      "  1.22923763e+26  2.90280894e+26  1.87145604e+25  1.79484444e+26\n",
      "  3.06455197e+26  1.05219213e+26  2.31025902e+24  1.58866405e+24\n",
      "  9.06540916e+25  4.95242049e+24 -4.04415294e+23  1.52008733e+26\n",
      "  2.69004769e+24  2.72962105e+26  3.17766104e+26  2.58835910e+26\n",
      "  2.47201429e+26  2.47202792e+26  3.08043921e+26  3.06453890e+26\n",
      "  3.06452269e+26  2.98205586e+26]\n",
      "[-5.98037321e+26  6.07141132e+26  8.75499283e+25 -1.94607589e+27\n",
      " -2.32507252e+27 -2.25616567e+27 -2.32491668e+27  1.10023379e+27\n",
      " -9.32633285e+26 -2.20238640e+27 -1.41988998e+26 -1.36176409e+27\n",
      " -2.32510224e+27 -7.98307320e+26 -1.75281361e+25 -1.20533324e+25\n",
      " -6.87800479e+26 -3.75744451e+25  3.06833402e+24 -1.15330348e+27\n",
      " -2.04096259e+25 -2.07098724e+27 -2.41091908e+27 -1.96381057e+27\n",
      " -1.87553875e+27 -1.87554909e+27 -2.33715603e+27 -2.32509233e+27\n",
      " -2.32508002e+27 -2.26251173e+27]\n",
      "[ 4.53736119e+27 -4.60643259e+27 -6.64248923e+26  1.47650471e+28\n",
      "  1.76405275e+28  1.71177252e+28  1.76393451e+28 -8.34756951e+27\n",
      "  7.07596987e+27  1.67096972e+28  1.07728288e+27  1.03318226e+28\n",
      "  1.76407530e+28  6.05682709e+27  1.32987494e+26  9.14496813e+25\n",
      "  5.21840207e+27  2.85080584e+26 -2.32797172e+25  8.75021387e+27\n",
      "  1.54849607e+26  1.57127603e+28  1.82918528e+28  1.48996016e+28\n",
      "  1.42298757e+28  1.42299541e+28  1.77322061e+28  1.76406778e+28\n",
      "  1.76405844e+28  1.71658733e+28]\n",
      "[-3.44253540e+28  3.49494048e+28  5.03971436e+27 -1.12023697e+29\n",
      " -1.33840217e+29 -1.29873670e+29 -1.33831246e+29  6.33337361e+28\n",
      " -5.36859990e+28 -1.26777926e+29 -8.17343892e+27 -7.83884370e+28\n",
      " -1.33841927e+29 -4.59536740e+28 -1.00898768e+27 -6.93836688e+26\n",
      " -3.95924705e+28 -2.16293119e+27  1.76625239e+26 -6.63886339e+28\n",
      " -1.17485744e+27 -1.19214080e+29 -1.38781878e+29 -1.13044574e+29\n",
      " -1.07963305e+29 -1.07963900e+29 -1.34535790e+29 -1.33841357e+29\n",
      " -1.33840649e+29 -1.30238974e+29]\n",
      "[ 2.61188156e+29 -2.65164175e+29 -3.82367513e+28  8.49933534e+29\n",
      "  1.01545737e+30  9.85362835e+29  1.01538930e+30 -4.80518565e+29\n",
      "  4.07320345e+29  9.61875156e+29  6.20125921e+28  5.94739891e+29\n",
      "  1.01547035e+30  3.48654523e+29  7.65527734e+27  5.26419931e+27\n",
      "  3.00391518e+29  1.64103471e+28 -1.34007106e+27  5.03696341e+29\n",
      "  8.91374560e+27  9.04487594e+29  1.05295018e+30  8.57679017e+29\n",
      "  8.19126985e+29  8.19131500e+29  1.02073474e+30  1.01546602e+30\n",
      "  1.01546064e+30  9.88134427e+29]\n",
      "[-1.98165726e+30  2.01182366e+30  2.90105559e+29 -6.44851966e+30\n",
      " -7.70436339e+30 -7.47603356e+30 -7.70384698e+30  3.64573616e+30\n",
      " -3.09037489e+30 -7.29783049e+30 -4.70494931e+29 -4.51234329e+30\n",
      " -7.70446187e+30 -2.64527220e+30 -5.80812552e+28 -3.99399382e+28\n",
      " -2.27909658e+30 -1.24506731e+29  1.01672357e+28 -3.82158796e+30\n",
      " -6.76293633e+28 -6.86242606e+30 -7.98882466e+30 -6.50728531e+30\n",
      " -6.21478770e+30 -6.21482195e+30 -7.74440331e+30 -7.70442902e+30\n",
      " -7.70438825e+30 -7.49706186e+30]\n",
      "[ 1.50350061e+31 -1.52638811e+31 -2.20105611e+30  4.89254797e+31\n",
      "  5.84536753e+31  5.67213170e+31  5.84497573e+31 -2.76605174e+31\n",
      "  2.34469431e+31  5.53692748e+31  3.56968598e+30  3.42355411e+31\n",
      "  5.84544224e+31  2.00699103e+31  4.40667536e+29  3.03027786e+29\n",
      "  1.72917040e+31  9.44643401e+29 -7.71397017e+28  2.89947203e+31\n",
      "  5.13109863e+29  5.20658235e+31  6.06119077e+31  4.93713398e+31\n",
      "  4.71521349e+31  4.71523948e+31  5.87574617e+31  5.84541733e+31\n",
      "  5.84538639e+31  5.68808606e+31]\n",
      "[-1.14071899e+32  1.15808394e+32  1.66996042e+31 -3.71201871e+32\n",
      " -4.43493120e+32 -4.30349567e+32 -4.43463393e+32  2.09862752e+32\n",
      " -1.77893997e+32 -4.20091505e+32 -2.70835181e+31 -2.59748029e+32\n",
      " -4.43498788e+32 -1.52272155e+32 -3.34338293e+30 -2.29909817e+30\n",
      " -1.31193663e+32 -7.16709166e+30  5.85265628e+29 -2.19985465e+32\n",
      " -3.89300917e+30 -3.95027933e+32 -4.59867817e+32 -3.74584650e+32\n",
      " -3.57747350e+32 -3.57749322e+32 -4.45797973e+32 -4.43496898e+32\n",
      " -4.43494551e+32 -4.31560038e+32]\n",
      "[ 8.65473421e+32 -8.78648362e+32 -1.26701350e+32  2.81634089e+33\n",
      "  3.36482088e+33  3.26509960e+33  3.36459535e+33 -1.59224696e+33\n",
      "  1.34969724e+33  3.18727079e+33  2.05485008e+32  1.97073089e+33\n",
      "  3.36486389e+33  1.15530209e+33  2.53665372e+31  1.74434578e+31\n",
      "  9.95377734e+32  5.43773480e+31 -4.44046123e+30  1.66904886e+33\n",
      "  2.95365993e+31  2.99711129e+33  3.48905713e+33  2.84200633e+33\n",
      "  2.71426026e+33  2.71427523e+33  3.38230801e+33  3.36484955e+33\n",
      "  3.36483174e+33  3.27428355e+33]\n",
      "[-6.56642214e+33  6.66638156e+33  9.61294165e+32 -2.13678233e+34\n",
      " -2.55291888e+34 -2.47725947e+34 -2.55274776e+34  1.20805162e+34\n",
      " -1.02402703e+34 -2.41821008e+34 -1.55903263e+33 -1.49521067e+34\n",
      " -2.55295151e+34 -8.76537746e+33 -1.92458124e+32 -1.32345032e+32\n",
      " -7.55201746e+33 -4.12565670e+32  3.36901657e+31 -1.26632189e+34\n",
      " -2.24096748e+32 -2.27393441e+34 -2.64717800e+34 -2.15625493e+34\n",
      " -2.05933288e+34 -2.05934423e+34 -2.56618651e+34 -2.55294063e+34\n",
      " -2.55292711e+34 -2.48422741e+34]\n",
      "[ 4.98200160e+34 -5.05784168e+34 -7.29342246e+33  1.62119534e+35\n",
      "  1.93692176e+35  1.87951831e+35  1.93679193e+35 -9.16559272e+34\n",
      "  7.76938220e+34  1.83471703e+35  1.18285162e+34  1.13442935e+35\n",
      "  1.93694652e+35  6.65036814e+34  1.46019653e+33  1.00411327e+33\n",
      "  5.72978135e+34  3.13017163e+33 -2.55610218e+32  9.60769437e+34\n",
      "  1.70024152e+33  1.72525382e+35  2.00843698e+35  1.63596937e+35\n",
      "  1.56243377e+35  1.56244238e+35  1.94698803e+35  1.93693826e+35\n",
      "  1.93692801e+35  1.88480495e+35]\n",
      "[-3.77988796e+35  3.83742849e+35  5.53358307e+34 -1.23001501e+36\n",
      " -1.46955939e+36 -1.42600690e+36 -1.46946089e+36  6.95401494e+35\n",
      " -5.89469786e+35 -1.39201577e+36 -8.97439814e+34 -8.60701413e+35\n",
      " -1.46957817e+36 -5.04569216e+35 -1.10786381e+34 -7.61829476e+33\n",
      " -4.34723496e+35 -2.37488845e+34  1.93933696e+33 -7.28944131e+35\n",
      " -1.28998804e+34 -1.30896508e+36 -1.52381861e+36 -1.24122419e+36\n",
      " -1.18543209e+36 -1.18543863e+36 -1.47719676e+36 -1.46957191e+36\n",
      " -1.46956413e+36 -1.43001791e+36]\n",
      "[ 2.86783388e+36 -2.91149038e+36 -4.19837761e+35  9.33223090e+36\n",
      "  1.11496750e+37  1.08192384e+37  1.11489276e+37 -5.27607163e+36\n",
      "  4.47235855e+36  1.05613447e+37  6.80895394e+35  6.53021650e+36\n",
      "  1.11498175e+37  3.82821054e+36  8.40545913e+34  5.78006651e+34\n",
      "  3.29828498e+36  1.80184852e+35 -1.47139182e+34  5.53056254e+36\n",
      "  9.78725146e+34  9.93123196e+36  1.15613443e+37  9.41727595e+36\n",
      "  8.99397642e+36  8.99402600e+36  1.12076203e+37  1.11497699e+37\n",
      "  1.11497109e+37  1.08496703e+37]\n",
      "[-2.17585051e+37  2.20897308e+37  3.18534562e+36 -7.08044477e+37\n",
      " -8.45935539e+37 -8.20865028e+37 -8.45878837e+37  4.00300145e+37\n",
      " -3.39321734e+37 -8.01298413e+37 -5.16601259e+36 -4.95453207e+37\n",
      " -8.45946351e+37 -2.90449664e+37 -6.37729496e+35 -4.38538674e+35\n",
      " -2.50243751e+37 -1.36707815e+36  1.11635778e+35 -4.19608591e+37\n",
      " -7.42567283e+35 -7.53491208e+37 -8.77169254e+37 -7.14496919e+37\n",
      " -6.82380815e+37 -6.82384577e+37 -8.50331903e+37 -8.45942745e+37\n",
      " -8.45938268e+37 -8.23173926e+37]\n",
      "[ 1.65083671e+38 -1.67596709e+38 -2.41674943e+37  5.37199505e+38\n",
      "  6.41818652e+38  6.22797438e+38  6.41775632e+38 -3.03711202e+38\n",
      "  2.57446352e+38  6.07952077e+38  3.91949870e+37  3.75904659e+38\n",
      "  6.41826856e+38  2.20366687e+38  4.83850917e+36  3.32723107e+36\n",
      "  1.89862111e+38  1.03721409e+37 -8.46990358e+35  3.18360688e+38\n",
      "  5.63392257e+36  5.71680335e+38  6.65515944e+38  5.42095028e+38\n",
      "  5.17728261e+38  5.17731115e+38  6.45154212e+38  6.41824120e+38\n",
      "  6.41820723e+38  6.24549219e+38]\n",
      "[-1.25250418e+39  1.27157081e+39  1.83360882e+38 -4.07577938e+39\n",
      " -4.86953395e+39 -4.72521834e+39 -4.86920755e+39  2.30428331e+39\n",
      " -1.95326787e+39 -4.61258530e+39 -2.97375776e+38 -2.85202135e+39\n",
      " -4.86959619e+39 -1.67194123e+39 -3.67101900e+37 -2.52439916e+37\n",
      " -1.44050035e+39 -7.86943351e+37  6.42618953e+36 -2.41543024e+39\n",
      " -4.27450607e+37 -4.33738844e+39 -5.04932736e+39 -4.11292214e+39\n",
      " -3.92804936e+39 -3.92807101e+39 -4.89484113e+39 -4.86957543e+39\n",
      " -4.86954966e+39 -4.73850926e+39]\n",
      "[ 9.50285815e+39 -9.64751838e+39 -1.39117496e+39  3.09232927e+40\n",
      "  3.69455777e+40  3.58506426e+40  3.69431013e+40 -1.74827980e+40\n",
      "  1.48196132e+40  3.49960859e+40  2.25621588e+39  2.16385341e+40\n",
      "  3.69460499e+40  1.26851635e+40  2.78523405e+38  1.91528361e+38\n",
      "  1.09292015e+40  5.97060766e+38 -4.87560590e+37  1.83260794e+40\n",
      "  3.24310494e+38  3.29081434e+40  3.83096860e+40  3.12050981e+40\n",
      "  2.98024522e+40  2.98026165e+40  3.71375855e+40  3.69458925e+40\n",
      "  3.69456969e+40  3.59514820e+40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.20990115e+40  7.31965613e+40  1.05549655e+40 -2.34617712e+41\n",
      " -2.80309312e+41 -2.72001945e+41 -2.80290524e+41  1.32643509e+41\n",
      " -1.12437695e+41 -2.65518349e+41 -1.71181062e+40 -1.64173441e+41\n",
      " -2.80312895e+41 -9.62434393e+40 -2.11318131e+39 -1.45314233e+39\n",
      " -8.29208026e+40 -4.52995198e+39  3.69916462e+38 -1.39041558e+41\n",
      " -2.46057193e+39 -2.49676947e+41 -2.90658920e+41 -2.36755794e+41\n",
      " -2.26113798e+41 -2.26115044e+41 -2.81766092e+41 -2.80311700e+41\n",
      " -2.80310217e+41 -2.72767021e+41]\n",
      "[ 5.47021472e+41 -5.55348678e+41 -8.00814414e+40  1.78006499e+42\n",
      "  2.12673114e+42  2.06370242e+42  2.12658859e+42 -1.00637784e+42\n",
      "  8.53074573e+41  2.01451081e+42  1.29876561e+41  1.24559818e+42\n",
      "  2.12675832e+42  7.30207347e+41  1.60328904e+40  1.10251173e+40\n",
      "  6.29127343e+41  3.43691397e+40 -2.80658838e+39  1.05492040e+42\n",
      "  1.86685733e+40  1.89432072e+42  2.20525452e+42  1.79628681e+42\n",
      "  1.71554505e+42  1.71555451e+42  2.13778385e+42  2.12674926e+42\n",
      "  2.12673800e+42  2.06950712e+42]\n",
      "w is [-3.38323072e+41  3.43473301e+41  4.95289501e+40 -1.10093860e+42\n",
      " -1.31534546e+42 -1.27636332e+42 -1.31525730e+42  6.22426834e+41\n",
      " -5.27611482e+41 -1.24593918e+42 -8.03263478e+40 -7.70380364e+41\n",
      " -1.31536227e+42 -4.51620284e+41 -9.91605816e+39 -6.81883937e+39\n",
      " -3.89104095e+41 -2.12567029e+40  1.73582510e+39 -6.52449544e+41\n",
      " -1.15461813e+40 -1.17160374e+42 -1.36391078e+42 -1.11097151e+42\n",
      " -1.06103417e+42 -1.06104002e+42 -1.32218137e+42 -1.31535667e+42\n",
      " -1.31534971e+42 -1.27995342e+42] \n",
      " of shape (30,)\n",
      "\n",
      " loss of least squares is 1.5598852896067792e+86\n"
     ]
    }
   ],
   "source": [
    "w_ls_gd, loss_ls_gd = least_squares_GD(y_train, x_train, w_initial, max_iters, gamma)\n",
    "print(\"w is {} \\n of shape {}\".format(w_ls_gd, w_ls_gd.shape))\n",
    "print(\"\\n loss of least squares is {}\".format(loss_ls_gd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Stochastic Gradient Descent algorithm with least squares.\"\"\"\n",
    "    \"\"\"Required by project description\"\"\"\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad = compute_gradient_least_squares(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "    loss = compute_loss_ls(y, tx, w)                                 #TO CHECK p.3 least squares\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ls_sgd, loss_ls_sgd = least_squares_SGD(y_train, x_train, w_initial, max_iters, gamma)\n",
    "print(\"w is {} \\n of shape {}\".format(w_ls_sgd, w_ls_sgd.shape))\n",
    "print(\"\\n loss of least squares is {}\".format(loss_ls_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should do grid search  TO DO\n",
    "lambda_ = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Je suis un peu confuse. Est ce que ce qu'il veut c'est ça ou bien ce que j'ai fait aux fonctions ridge_GD et ridge_SGD ?? \n",
    "# ridge_GD et ridge_SGD sont juste en-dessous\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"Ridge regression.\"\"\"\n",
    "    \"\"\"Required by project description\"\"\"\n",
    "    lambd = 2 * tx.shape[0] * lambda_\n",
    "    a = tx.T.dot(tx) + lambd * np.identity(tx.shape[1])\n",
    "    b = tx.T.dot(y) \n",
    "    w = np.linalg.solve(a, b) \n",
    "    err = y - tx.dot(w)\n",
    "    loss = (1/2) * np.mean(err**2) + lambda_ * (np.linalg.norm(w,2))**2   #TO CHECK p.3 ridge regression\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w is [ 0.03572402 -0.24077855 -0.21610016 -0.01054923 -0.01919922  0.34814992\n",
      " -0.03018983  0.23504977 -0.01013674 -0.00087247 -0.15674991  0.11455458\n",
      " -0.02161131  0.18503225 -0.00076269 -0.00130455  0.23850622 -0.00086979\n",
      "  0.00249252  0.10383837  0.00113401 -0.06198121 -0.14991608  0.03645409\n",
      "  0.04367213  0.0436772  -0.02328488 -0.02309077 -0.0234825  -0.09703028] \n",
      " of shape (30,)\n",
      "\n",
      " loss of ridge regression is 0.39491577663267774\n"
     ]
    }
   ],
   "source": [
    "w_rreg, loss_rreg = ridge_regression(y_train, x_train, lambda_)\n",
    "print(\"w is {} \\n of shape {}\".format(w_rreg, w_rreg.shape))\n",
    "print(\"\\n loss of ridge regression is {}\".format(loss_rreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_ridge(y, tx, w, lambda_):\n",
    "    \"\"\"Not required specifically\"\"\" \n",
    "    err = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(err) / len(err) + (2 * lambda_* w)\n",
    "    return grad, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_GD(y, tx, initial_w, max_iters, gamma, lambda_):\n",
    "    \"\"\"Gradient descent algorithm with Ridge.\"\"\"\n",
    "    \"\"\"Not required specifically\"\"\" \n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and error\n",
    "        grad, _ = compute_gradient_ridge(y, tx, w, lambda_)\n",
    "        #print(\"grad is {} \\n of shape {}\".format(grad, grad.shape))\n",
    "        #print(\"\\n err of least squares is {}\".format(err))\n",
    "        # gradient w by descent update\n",
    "        w = w - gamma * grad\n",
    "        # calculate loss\n",
    "    loss = 1/2*np.mean((y - tx.dot(w))**2)            \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w is [-3.70387313e+41  3.76025650e+41  5.42230084e+40 -1.20527899e+42\n",
      " -1.44000606e+42 -1.39732941e+42 -1.43990953e+42  6.81416734e+41\n",
      " -5.77615350e+41 -1.36402186e+42 -8.79391996e+40 -8.43392417e+41\n",
      " -1.44002446e+42 -4.94422159e+41 -1.08558430e+40 -7.46508827e+39\n",
      " -4.25981059e+41 -2.32712864e+40  1.90033624e+39 -7.14284818e+41\n",
      " -1.26404594e+40 -1.28264135e+42 -1.49317410e+42 -1.21626276e+42\n",
      " -1.16159265e+42 -1.16159906e+42 -1.44748983e+42 -1.44001832e+42\n",
      " -1.44001070e+42 -1.40125977e+42] \n",
      " of shape (30,)\n",
      "\n",
      " loss of ridge_GD is 1.869569596937695e+86\n"
     ]
    }
   ],
   "source": [
    "w, loss = ridge_GD(y_train, x_train, w_initial, max_iters, gamma, lambda_)\n",
    "print(\"w is {} \\n of shape {}\".format(w, w.shape))\n",
    "print(\"\\n loss of ridge_GD is {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_SGD(y, tx, initial_w, max_iters, gamma, lambda_):\n",
    "    \"\"\"Stochastic Gradient Descent algorithm with least squares.\"\"\"\n",
    "    \"\"\"Not required specifically\"\"\" #Implicitely requested I think\n",
    "    \n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad, _ = compute_gradient_ridge(y_batch, tx_batch, w, lambda_)    \n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "    loss = np.mean((y - tx.dot(w))**2)/2 + lambda_ * (np.linalg.norm(w,2))**2\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, loss = ridge_SGD(y_train, x_train, w_initial, max_iters, gamma, lambda_)\n",
    "print(\"w is {} \\n of shape {}\".format(w, w.shape))\n",
    "print(\"\\n loss of ridge SGD is {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-4, -0.05, 15)\n",
    "\n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    w, loss_tr = ridge_SGD(y_train, x_train, w_initial, max_iters, gamma, lambda_)\n",
    "    rmse_tr.append(np.sqrt(2*loss_tr))\n",
    "    loss_te = (1/2) * np.mean((y_test - x_test.dot(w))**2) + lambda_ * (np.linalg.norm(w,2))**2\n",
    "    rmse_te.append(np.sqrt(2*loss_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search to choose lambda\n",
    "plot_train_test(rmse_tr, rmse_te, lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "Debugged but not sure from here: Il y aura probablement une correction de labo pour améliorer / re-vérifier les fonctions de logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(tx, w):\n",
    "    \"\"\"Compute sigmoid function\"\"\"\n",
    "    z = np.array(np.exp(-tx.dot(w)))\n",
    "    return 1./(1 + z)\n",
    "\n",
    "value = sigmoid(x_train, w_initial)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logreg_loss(y, tx, w):  #np.log parce que math.log fonctionne pas.. J'ai toujours pas compris pourquoi.\n",
    "    \"\"\"Compute error and gradient of logistic regression\"\"\"\n",
    "    sig = sigmoid(tx, w)\n",
    "    loss = sum(-y * np.log(sig) - (1-y) * np.log(1-sig))/len(y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = compute_logreg_loss(y_train, x_train, w_initial)\n",
    "print(\"loss is {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logreg_grad(y, tx, w):\n",
    "    \"\"\"Compute error and gradient of logistic regression\"\"\" \n",
    "    sig = sigmoid(tx, w)\n",
    "    err  = sig - y\n",
    "    grad = tx.T.dot(err)/len(y)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = compute_logreg_grad(y_train, x_train, w_initial)\n",
    "print(\"grad is {} \\n of shape {}\".format(grad, grad.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma): #SGD  (GD easy to implement from here)\n",
    "    \"\"\"Stochastic Gradient Descent algorithm with logistic regression.\"\"\"\n",
    "    \"\"\"Required by project description\"\"\"\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient\n",
    "            grad = compute_logreg_grad(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # compute a stochastic loss\n",
    "            compute_logreg_loss(y_batch, tx_batch, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, loss = logistic_regression(y_train, x_train, w_initial, max_iters, gamma)\n",
    "print(\"w is {} \\n of shape {}\".format(w, w.shape))\n",
    "print(\"\\n loss of ridge_SGD is {}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logreg_reg_loss(y, tx, w, lambda_):\n",
    "    \"\"\"Compute error and gradient of logistic regression\"\"\"\n",
    "    reg = ( lambda_/(2*len(y)) ) * sum(w**2)\n",
    "    loss = compute_logreg_loss(y, tx, w) + reg\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = compute_logreg_reg_loss(y_train, x_train, w_initial, lambda_)\n",
    "print(\"loss is {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logreg_reg_grad(y, tx, w, lambda_):\n",
    "    \"\"\"Compute error and gradient of logistic regression\"\"\"     \n",
    "    grad = compute_logreg_grad(y, tx, w) \n",
    "    reg = (lambda_/len(y)) * w[1:]\n",
    "    grad[1:] = grad[1:] + reg            \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = compute_logreg_reg_grad(y_train, x_train, w_initial, lambda_)\n",
    "print(\"grad is {} \\n of shape {}\".format(grad, grad.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# La loss de ridge SGD me semble particulièrement haute. A vérifier avec les plots (et/ou code).\n",
    "\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    \"\"\"Stochastic Gradient Descent algorithm with REGULARIZED logistic regression.\"\"\"\n",
    "    \"\"\"Required by project description\"\"\"\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient\n",
    "            grad = compute_logreg_reg_grad(y_batch, tx_batch, w, lambda_)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # compute a stochastic loss\n",
    "            loss = compute_logreg_reg_loss(y_batch, tx_batch, w, lambda_)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, loss = reg_logistic_regression(y_train, x_train, lambda_, w_initial, max_iters, gamma)\n",
    "print(\"w is {} \\n of shape {}\".format(w, w.shape))\n",
    "print(\"\\n loss of ridge_SGD is {}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction de chaque méthode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = predict_labels(weights, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choisir méthode:\n",
    "(D'après moi mais hésitez pas à faire autrement si vous avez une meilleure idée)\n",
    "- Pour chaque méthode: faire un plot de test_error (=loss) pour un grand nombre d'iteration (assez pour que on voit que l'erreur commence à remonter si possible). \n",
    "- Garder la valeur de n iter ou la test error était minimale (au cas où: risque d'être très long pour ridge si lambda_ trop grand)\n",
    "- Comparer les loss de chaque méthode à leur n_iter optimale\n",
    "- La meilleure est celle ou c'est le plus faible\n",
    "\n",
    "En plus comme ça on aura des beau plots et des arguments pour le rapport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
