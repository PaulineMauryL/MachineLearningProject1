{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import load_csv_data, predict_labels, create_csv_submission\n",
    "from preprocessing import standardize_train, standardize_test, add_bias\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "(250000, 30)\n"
     ]
    }
   ],
   "source": [
    "y_train, input_train, ids_train = load_csv_data('train.csv', sub_sample=False)\n",
    "print(y_train.shape)\n",
    "print(input_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238,)\n",
      "(568238, 30)\n"
     ]
    }
   ],
   "source": [
    "y_test, input_test, ids_test = load_csv_data('test.csv', sub_sample=False)\n",
    "print(y_test.shape)\n",
    "print(input_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove -999 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEE: on pourrait ausi tenter build_polynomial comme dans les séries si on est motivés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_no_999, y_train_no_999 = remove_999(input_train, y_train)\n",
    "\n",
    "x_train_no_999, mean, std = standardize_train(x_train_no_999)\n",
    "x_train_no_999 = add_bias(x_train_no_999)\n",
    "\n",
    "x_test = standardize_test(input_test, mean, std)\n",
    "x_test = add_bias(x_test)   #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68114, 31)\n",
      "(568238, 31)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Should be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#besoin pour SGD, prise de lab02\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celle du lab03, juste sans les degrés\n",
    "def plot_train_test(train_errors, test_errors, lambdas):\n",
    "    plt.semilogx(lambdas, train_errors, color='b', marker='*', label=\"Train error\")\n",
    "    plt.semilogx(lambdas, test_errors, color='r', marker='*', label=\"Test error\")\n",
    "    plt.xlabel(\"lambda\")\n",
    "    #plt.ylabel(\"RMSE\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Ridge regression for polynomial degree \")\n",
    "    leg = plt.legend(loc=1, shadow=True)\n",
    "    leg.draw_frame(False)\n",
    "    #plt.savefig(\"ridge_regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods to implement \n",
    "P.2 of project description\n",
    "\n",
    "-  least_square\n",
    "-  least_square_GD\n",
    "-  least_square_SGD\n",
    "\n",
    "-  ridge_regression\n",
    "-  logistic_regression\n",
    "-  reg_logistic_regression\n",
    "\n",
    "All function should return (w, loss) <br>\n",
    "Later add them in implementations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm. (probably change afterwards)\n",
    "max_iters = 200                                          \n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.random.rand(x_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least square "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_ls(y, tx, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "    \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    #print(e)\n",
    "    return (1/2)*np.mean(e**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = compute_loss_ls(y_train, x_train, w_initial)\n",
    "#print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"Compute the optimal w and the loss with least square technique\"\"\"\n",
    "    \"\"\"Required by project description\"\"\"\n",
    "    a = tx.T.dot(tx) \n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)                #p.7 du cours least squares\n",
    "    loss = compute_loss_ls(y, tx, w)                 #p.3 du cours least squares\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w is [-3.14664000e-01  2.93788272e-02 -2.52531475e-01 -2.54791124e-01\n",
      " -3.03696824e-02 -1.40144726e+00  2.95701641e-01 -1.07889472e+01\n",
      "  2.67880862e-01 -2.44934971e-03 -3.28827529e+02 -1.82647888e-01\n",
      "  1.14039627e-01  2.05045961e+01  6.38853340e+01 -3.18961885e-04\n",
      " -1.80884297e-03  6.29944860e+01 -4.48641524e-04  1.54379296e-03\n",
      "  1.21462701e-01  3.95268757e-04 -6.33223475e-02 -2.06747093e-01\n",
      " -1.16655767e-01  9.86256328e-02  1.67907718e-01 -3.35146253e-02\n",
      " -2.98358685e+00 -5.36388099e+00  2.78478918e+02] \n",
      " of shape (31,)\n",
      "\n",
      " loss of least squares is 0.33944559848933104\n"
     ]
    }
   ],
   "source": [
    "w_ls, loss_ls = least_squares(y_train, x_train)\n",
    "print(\"w is {} \\n of shape {}\".format(w_ls, w_ls.shape))\n",
    "print(\"\\n loss of least squares is {}\".format(loss_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_least_squares(y, tx, w):\n",
    "    \"\"\"Compute the gradient of least square.\"\"\"\n",
    "    '''\n",
    "    print(\"y is {}\".format(y.shape))\n",
    "    print(\"tx is {}\".format(tx.shape))\n",
    "    print(\"w is {}\".format(w.shape))\n",
    "    a = tx.dot(w)\n",
    "    print(\"a is {}\".format(a.shape))\n",
    "    '''\n",
    "    err = y - w.dot(tx.T) #tx.dot(w)\n",
    "    #print(err)\n",
    "    #print(err.shape)\n",
    "    grad = -tx.T.dot(err)/len(err)         #p.5 du cours least squares\n",
    "    return grad                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad is [ 0.57961034  1.77414577 -0.33939429  0.91283755  5.05389876  6.09099835\n",
      "  5.83435243  6.09203034 -1.91261717  3.19426975  5.85219796  1.22995334\n",
      "  3.5394617   6.0914679   1.77543271  0.51719589  0.21840713  2.58515579\n",
      "  0.70416239  0.85919932  3.50541424  0.78169237  5.56387894  6.42859867\n",
      "  5.17323323  4.95835178  4.95687385  6.12262134  6.09169066  6.09123954\n",
      "  5.92050984] \n",
      " of shape (31,)\n"
     ]
    }
   ],
   "source": [
    "#compute_gradient_least_square(y_train, x_train, w)\n",
    "grad_ls = compute_gradient_least_squares(y_train, x_train, w_initial)\n",
    "print(\"grad is {} \\n of shape {}\".format(grad_ls, grad_ls.shape))\n",
    "#print(\"\\nerr of least squares is {}\".format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.1\n",
    "def least_squares_GD(y, tx, w_initial, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm with least squares.\"\"\"\n",
    "    \"\"\"Required by project description\"\"\"\n",
    "    w = w_initial\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and error\n",
    "        grad = compute_gradient_least_squares(y, tx, w)\n",
    "        #print(grad)\n",
    "        # gradient w by descent update\n",
    "        w = w - (gamma * grad)\n",
    "        # print(w)\n",
    "        # calculate loss    \n",
    "    loss = compute_loss_ls(y, tx, w)           \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w is [-0.314664    0.02358441 -0.23819999 -0.25217892  0.02891371 -0.30349336\n",
      "  0.44807777  0.24192151  0.27693041 -0.02272495 -0.0035236  -0.16347312\n",
      "  0.11111952  0.15236211  0.19513764 -0.00065166 -0.00112557  0.25392454\n",
      " -0.00077372  0.00238736  0.09579818  0.0008514  -0.04141987  0.10213103\n",
      "  0.50571422 -0.13920452 -0.32519927 -0.31898239  0.03988202 -0.15732273\n",
      " -0.22661771] \n",
      " of shape (31,)\n",
      "\n",
      " loss of least squares is 0.3420666358033106\n"
     ]
    }
   ],
   "source": [
    "w_ls_gd, loss_ls_gd = least_squares_GD(y_train, x_train, w_initial, max_iters, gamma)\n",
    "print(\"w is {} \\n of shape {}\".format(w_ls_gd, w_ls_gd.shape))\n",
    "print(\"\\n loss of least squares is {}\".format(loss_ls_gd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.01\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Stochastic Gradient Descent algorithm with least squares.\"\"\"\n",
    "    \"\"\"Required by project description\"\"\"\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad = compute_gradient_least_squares(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "    loss = compute_loss_ls(y, tx, w)                                 #TO CHECK p.3 least squares\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w is [-0.28032291  0.16570215  0.07666079 -0.25266048  0.04004951 -0.31582698\n",
      "  0.40969289  0.23024454  0.399266    0.21669759 -0.14169261 -0.06445322\n",
      "  0.20137663  0.13997472  0.28989413 -0.09077684  0.17035135  0.00634319\n",
      "  0.14447043  0.03204641 -0.06508943  0.06896809  0.03411254  0.25646954\n",
      "  0.44379835 -0.18862806 -0.37422311 -0.3594323   0.02784728 -0.16862433\n",
      " -0.35565734] \n",
      " of shape (31,)\n",
      "\n",
      " loss of least squares is 0.5125444943059494\n"
     ]
    }
   ],
   "source": [
    "w_ls_sgd, loss_ls_sgd = least_squares_SGD(y_train, x_train, w_initial, max_iters, gamma)\n",
    "print(\"w is {} \\n of shape {}\".format(w_ls_sgd, w_ls_sgd.shape))\n",
    "print(\"\\n loss of least squares is {}\".format(loss_ls_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should do grid search  TO DO\n",
    "lambda_ = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_ridge(y, tx, w, lambda_):\n",
    "    \"\"\"Not required specifically\"\"\" \n",
    "    err = y - tx.dot(w)\n",
    "    loss = (1/2) * np.mean(err**2) + lambda_ * (np.linalg.norm(w,2))**2   #TO CHECK p.3 ridge regression\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_ridge(y, tx, w, lambda_):\n",
    "    \"\"\"Not required specifically\"\"\" \n",
    "    err = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(err) / len(err) + (2 * lambda_* w)\n",
    "    return grad, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Je suis un peu confuse. Est ce que ce qu'il veut c'est ça ou bien ce que j'ai fait aux fonctions ridge_GD et ridge_SGD ?? \n",
    "# ridge_GD et ridge_SGD sont juste en-dessous\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"Ridge regression.\"\"\"\n",
    "    \"\"\"Required by project description\"\"\"\n",
    "    lambd = 2 * tx.shape[0] * lambda_\n",
    "    a = tx.T.dot(tx) + lambd * np.identity(tx.shape[1])\n",
    "    b = tx.T.dot(y) \n",
    "    w = np.linalg.solve(a, b) \n",
    "    loss = compute_loss_ridge(y, tx, w, lambda_)   \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_rreg, loss_rreg = ridge_regression(y_train, x_train, lambda_)\n",
    "#print(\"w is {} \\n of shape {}\".format(w_rreg, w_rreg.shape))\n",
    "#print(\"\\n loss of ridge regression is {}\".format(loss_rreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.1\n",
    "def ridge_GD(y, tx, initial_w, max_iters, gamma, lambda_):\n",
    "    \"\"\"Gradient descent algorithm with Ridge.\"\"\"\n",
    "    \"\"\"Not required specifically\"\"\" \n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute gradient and error\n",
    "        grad, _ = compute_gradient_ridge(y, tx, w, lambda_)\n",
    "        #print(\"grad is {} \\n of shape {}\".format(grad, grad.shape))\n",
    "        #print(\"\\n err of least squares is {}\".format(err))\n",
    "        # gradient w by descent update\n",
    "        w = w - gamma * grad\n",
    "        # calculate loss\n",
    "    loss = compute_loss_ridge(y, tx, initial_w, lambda_)            \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_r_gd, loss_r_gd = ridge_GD(y_train, x_train, w_initial, max_iters, gamma, lambda_)\n",
    "#print(\"w is {} \\n of shape {}\".format(w_r_gd, w_r_gd.shape))\n",
    "#print(\"\\n loss of ridge_GD is {}\".format(loss_r_gd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.01\n",
    "def ridge_SGD(y, tx, initial_w, max_iters, gamma, lambda_):\n",
    "    \"\"\"Stochastic Gradient Descent algorithm with least squares.\"\"\"\n",
    "    \"\"\"Not required specifically\"\"\" #Implicitely requested I think\n",
    "    \n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad, _ = compute_gradient_ridge(y_batch, tx_batch, w, lambda_)    \n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "    loss = compute_loss_ridge(y, tx, initial_w, lambda_) \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_r_sgd, loss_r_sgd = ridge_SGD(y_train, x_train, w_initial, max_iters, gamma, lambda_)\n",
    "#print(\"w is {} \\n of shape {}\".format(w_r_sgd, w_r_sgd.shape))\n",
    "#print(\"\\n loss of ridge SGD is {}\".format(loss_r_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "Debugged but not sure from here: Il y aura probablement une correction de labo pour améliorer / re-vérifier les fonctions de logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Il y a un NaN dans la loss à cause d'un outlier. Je sais pas si on choisis d'enlever ou comment traiter. On verra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99981039 0.90820056 0.92786083 ... 0.28142199 0.00709906 0.04907172]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(tx, w):\n",
    "    \"\"\"Compute sigmoid function\"\"\"\n",
    "    z = np.array(np.exp(-tx.dot(w)))\n",
    "    return 1./(1 + z)\n",
    "\n",
    "value = sigmoid(x_train, w_initial)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logreg_loss(y, tx, w):  #np.log parce que math.log fonctionne pas.. J'ai toujours pas compris pourquoi.\n",
    "    \"\"\"Compute error and gradient of logistic regression\"\"\"\n",
    "    sig = sigmoid(tx, w)\n",
    "    loss = np.sum((-y * np.log(sig) - (1-y) * np.log(1-sig)), axis = -1)/len(y)\n",
    "    print(max(sig))\n",
    "    #a = (-y * np.log(sig) - (1-y) * np.log(1-sig))\n",
    "    #print(max(a))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.51459227 0.10073464 0.53488351 0.42669814 0.02003378 0.140064\n",
      " 0.2264286  0.072293   0.49416143 0.6004996  0.14155126 0.00350222\n",
      " 0.81835964 0.2093738  0.7436304  0.88667924 0.88977018 0.11193385\n",
      " 0.70287969 0.70028424 0.34716633 0.66417114 0.94782936 0.73171242\n",
      " 0.5700489  0.53273724 0.82729083 0.43374625 0.09599914 0.22535177\n",
      " 0.38780099]\n",
      "1.0\n",
      "loss is nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pauli\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Pauli\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in multiply\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "loss = compute_logreg_loss(y_train, x_train, w_initial)\n",
    "print(\"loss is {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logreg_grad(y, tx, w):\n",
    "    \"\"\"Compute error and gradient of logistic regression\"\"\" \n",
    "    sig = sigmoid(tx, w)\n",
    "    err  = sig - y\n",
    "    grad = tx.T.dot(err)/len(y)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_lr = compute_logreg_grad(y_train, x_train, w_initial)\n",
    "print(\"grad is {} \\n of shape {}\".format(grad_lr, grad_lr.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.01\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma): #SGD  (GD easy to implement from here)\n",
    "    \"\"\"Stochastic Gradient Descent algorithm with logistic regression.\"\"\"\n",
    "    \"\"\"Required by project description\"\"\"\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient\n",
    "            grad = compute_logreg_grad(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # compute a stochastic loss\n",
    "            compute_logreg_loss(y_batch, tx_batch, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_lr, loss_lr = logistic_regression(y_train, x_train, w_initial, max_iters, gamma)\n",
    "print(\"w is {} \\n of shape {}\".format(w_lr, w_lr.shape))\n",
    "print(\"\\n loss of ridge_SGD is {}\".format(loss_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logreg_reg_loss(y, tx, w, lambda_):\n",
    "    \"\"\"Compute error and gradient of logistic regression\"\"\"\n",
    "    reg = ( lambda_/(2*len(y)) ) * sum(w**2)\n",
    "    loss = compute_logreg_loss(y, tx, w) + reg\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_lrr = compute_logreg_reg_loss(y_train, x_train, w_initial, lambda_)\n",
    "print(\"loss is {}\".format(loss_lrr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logreg_reg_grad(y, tx, w, lambda_):\n",
    "    \"\"\"Compute error and gradient of logistic regression\"\"\"     \n",
    "    grad = compute_logreg_grad(y, tx, w) \n",
    "    reg = (lambda_/len(y)) * w[1:]\n",
    "    grad[1:] = grad[1:] + reg            \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_lrr = compute_logreg_reg_grad(y_train, x_train, w_initial, lambda_)\n",
    "print(\"grad is {} \\n of shape {}\".format(grad_lrr, grad_lrr.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# La loss de ridge SGD me semble particulièrement haute. A vérifier avec les plots (et/ou code).\n",
    "gamma = 0.01\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    \"\"\"Stochastic Gradient Descent algorithm with REGULARIZED logistic regression.\"\"\"\n",
    "    \"\"\"Required by project description\"\"\"\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient\n",
    "            grad = compute_logreg_reg_grad(y_batch, tx_batch, w, lambda_)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # compute a stochastic loss\n",
    "            loss = compute_logreg_reg_loss(y_batch, tx_batch, w, lambda_)\n",
    "    return w_rlr_sgd, loss_rlr_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_rlr_sgd, loss_rlr_sgd = reg_logistic_regression(y_train, x_train, lambda_, w_initial, max_iters, gamma)\n",
    "print(\"w is {} \\n of shape {}\".format(w_rlr_sgd, w_rlr_sgd.shape))\n",
    "print(\"\\n loss of ridge_SGD is {}\".format(loss_rlr_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choisir méthode:\n",
    "(D'après moi)\n",
    "- Pour chaque méthode: faire un plot de test_error (=loss) pour un grand nombre d'iteration (assez pour que on voit que l'erreur commence à remonter si possible).  Il faudrait idealement voir la courbe descendre et monter dans le graphe (underfit à overfit)\n",
    "- Garder la valeur de n iter ou la test error était minimale (au cas où: risque d'être très long pour ridge si lambda_ trop grand)\n",
    "- Comparer les loss de chaque méthode à leur n_iter optimale\n",
    "- La meilleure est celle ou c'est le plus faible\n",
    "\n",
    "En plus comme ça on aura des beau plots et des arguments pour le rapport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = predict_labels(weights, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
