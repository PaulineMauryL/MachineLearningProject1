{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "\n",
    "import numpy as np\n",
    "from implementations import *\n",
    "\n",
    "#Useful tools\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generation of a data set (in order to check our fonctions)\n",
    "\n",
    "N = 60 #nb of datas\n",
    "M = 5 #nb of features\n",
    "\n",
    "y = np.zeros(N)\n",
    "tx = np.zeros((N,M))\n",
    "\n",
    "a,b = int(-N/2), int(N/2)\n",
    "\n",
    "for k in range(a,b):\n",
    "    #points measures with normal distributed noise\n",
    "    #noise on both measures tx and y\n",
    "    y[k] = 0.5 * k**4 + 2*k**2 - 8 + 3*np.random.randn()\n",
    "    tx[k] = [k**i for i in range(M)] + 0.1*np.random.randn(M)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.96914278,  9.95904049,  4.08007956, -0.01785012,  0.49696254])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test of least_squares_GD : il est bon pour les grands degrees mais pas le reste\n",
    "\n",
    "w_init = np.ones(M)\n",
    "w_init = [10 for _ in range(M)]\n",
    "max_iters = 10000000\n",
    "gamma = 0.00000000001\n",
    "\n",
    "w_LSGD, loss_LSGD = least_squares_GD(y, tx, w_init, max_iters, gamma)\n",
    "\n",
    "w_LSGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test of least_squares_GD : il est bon pour les grands degrees mais pas le reste\n",
    "#second test (without the minus)\n",
    "#ok ca diverge (j'ai rien dit)\n",
    "\n",
    "w_init = np.ones(M)\n",
    "w_init = [10 for _ in range(M)]\n",
    "max_iters = 10000000\n",
    "gamma = 0.00000000001\n",
    "\n",
    "w_LSGD, loss_LSGD = least_squares_GD(y, tx, w_init, max_iters, gamma)\n",
    "\n",
    "w_LSGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.31425783e+00,  1.64637385e-02,  1.99263335e+00, -4.44458146e-06,\n",
       "        5.00007877e-01])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test of least_squares : il fonctionne très bien\n",
    "w_LS, loss_LS = least_squares(y, tx)\n",
    "\n",
    "w_LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.99940254,  9.98432228,  9.87076257, -0.02441925,  0.48904404])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test of least_squares_SGD : il est bon pour les grands degrees mais pas le reste\n",
    "#Note : Faut faire plus d'itérations qu'avec GD et prendre un gamma plus petit\n",
    "\n",
    "w_init = [10 for _ in range(M)]\n",
    "max_iters = 1000000\n",
    "gamma = 0.000000000001\n",
    "\n",
    "w_LS_SGD, loss_LS_SGD = least_squares_SGD(y, tx, w_init, max_iters, gamma)\n",
    "w_LS_SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[352.26566529]\n",
      "[352.97501283]\n",
      "[1290.82825213]\n",
      "[3436.17327723]\n",
      "[229837.83697349]\n"
     ]
    }
   ],
   "source": [
    "#Test batch_iter : C'est juste pour montrer que de base, on prend qu'un seul \n",
    "#                  point à chaque fois ! donc faut vraiment bcp d'itérations\n",
    "#                  pour que ca converge\n",
    "max_iters = 5\n",
    "\n",
    "for n_iter in range(max_iters):\n",
    "    for y_batch, tx_batch in batch_iter(y, tx, batch_size=1, num_batches=1):\n",
    "        print(y_batch)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.78201210e-01,  1.01187972e-02,  1.96031584e+00, -6.39111201e-05,\n",
       "        5.00041726e-01])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################################################################################################\n",
    "##################################################################################################################\n",
    "##################################################################################################################\n",
    "\n",
    "#Test Ridge_regression \n",
    "#Globalement ca marche bien, le vrai souci, c'est que s'il y a une dépendance\n",
    "#constante (comme ici), elle est presque automatiquement détruite car elle a peu\n",
    "#d'influence sur les grandes valeurs\n",
    "\n",
    "lambda_ = 1\n",
    "w_RR, loss_RR = ridge_regression(y, tx, lambda_)\n",
    "\n",
    "w_RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.99936157,  9.98422825,  9.86965421, -0.02449436,  0.48868362])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test Ridge Regression with Gradient Descent !\n",
    "#Watch out of the types !\n",
    "#Our functions only works with 'np.array' !\n",
    "# 2 * [1,2,3] = [2,4,6] if type = 'np.array' \n",
    "# 2 * [1,2,3] = [1,2,3,1,2,3] if type = 'list'\n",
    "\n",
    "#C'est comme Least Square with Gradient Descent et on pénalise encore plus\n",
    "#les petits polynomes car on a un régulateur !\n",
    "\n",
    "\n",
    "w_init = np.array([10 for _ in range(M)])\n",
    "max_iters = 1000000\n",
    "gamma = 0.000000000001\n",
    "lambda_ = 1\n",
    "\n",
    "w_RR_GD, loss_RR_GD = ridge_GD(y, tx, w_init, max_iters, gamma,lambda_)\n",
    "w_RR_GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\etaxi\\Desktop\\ALL\\MASTER1_COURS\\GitHub\\MachineLearningProject1\\implementations.py:169: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Test Logistic_regression\n",
    "#j'ai un souci ?\n",
    "\n",
    "w_init = np.array([10 for _ in range(M)])\n",
    "max_iters = 10\n",
    "gamma = 0.000000001\n",
    "\n",
    "w_LR, loss_LR = logistic_regression(y, tx, w_init, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\etaxi\\Desktop\\ALL\\MASTER1_COURS\\GitHub\\MachineLearningProject1\\implementations.py:170: RuntimeWarning: divide by zero encountered in log\n",
      "  return loss\n"
     ]
    }
   ],
   "source": [
    "#Test reg_logistic_regression\n",
    "#j'ai un souci ?\n",
    "\n",
    "w_init = np.array([10 for _ in range(M)])\n",
    "max_iters = 10\n",
    "gamma = 0.000000001\n",
    "Lambda_ = 1\n",
    "\n",
    "w_RLR, loss_RLR = reg_logistic_regression(y, tx, lambda_, w_init, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
